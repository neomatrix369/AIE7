{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-IqJAMkwnCF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Advanced Retrieval with LangChain\n",
    "\n",
    "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
    "\n",
    "We'll touch on:\n",
    "\n",
    "- Naive Retrieval\n",
    "- Best-Matching 25 (BM25)\n",
    "- Multi-Query Retrieval\n",
    "- Parent-Document Retrieval\n",
    "- Contextual Compression (a.k.a. Rerank)\n",
    "- Ensemble Retrieval\n",
    "- Semantic chunking\n",
    "\n",
    "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
    "\n",
    "There will be two breakout rooms:\n",
    "\n",
    "- 🤝 Breakout Room Part #1\n",
    "  - Task 1: Getting Dependencies!\n",
    "  - Task 2: Data Collection and Preparation\n",
    "  - Task 3: Setting Up QDrant!\n",
    "  - Task 4-10: Retrieval Strategies\n",
    "- 🤝 Breakout Room Part #2\n",
    "  - Activity: Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rKP3hgHivpe"
   },
   "source": [
    "# 🤝 Breakout Room Part #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xes8oT-xHN7"
   },
   "source": [
    "## Task 1: Getting Dependencies!\n",
    "\n",
    "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7OHJXzfyJyA"
   },
   "source": [
    "We'll also provide our OpenAI key, as well as our Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_env_var_is_set(env_var_name: str, human_readable_string: str = \"API Key\"):\n",
    "    api_key = os.getenv(env_var_name)\n",
    "  \n",
    "    if api_key:\n",
    "       print(f\"{env_var_name} is present\")\n",
    "    else:\n",
    "      print(f\"{env_var_name} is NOT present, paste key at the prompt:\")\n",
    "      os.environ[env_var_name] = getpass.getpass(f\"Please enter your {human_readable_string}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LttlDQUYgSI",
    "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is present\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "\n",
    "check_if_env_var_is_set(\"OPENAI_API_KEY\", \"OpenAI API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iUahNiJyQbv",
    "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHERE_API_KEY is present\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")\n",
    "\n",
    "check_if_env_var_is_set(\"COHERE_API_KEY\", \"Cohere API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw304iAFyRtl"
   },
   "source": [
    "## Task 2: Data Collection and Preparation\n",
    "\n",
    "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A92NC2QZzCsi"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GshBjVRJZ6p8"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./data/complaints.csv\",\n",
    "    metadata_columns=[\n",
    "      \"Date received\", \n",
    "      \"Product\", \n",
    "      \"Sub-product\", \n",
    "      \"Issue\", \n",
    "      \"Sub-issue\", \n",
    "      \"Consumer complaint narrative\", \n",
    "      \"Company public response\", \n",
    "      \"Company\", \n",
    "      \"State\", \n",
    "      \"ZIP code\", \n",
    "      \"Tags\", \n",
    "      \"Consumer consent provided?\", \n",
    "      \"Submitted via\", \n",
    "      \"Date sent to company\", \n",
    "      \"Company response to consumer\", \n",
    "      \"Timely response?\", \n",
    "      \"Consumer disputed?\", \n",
    "      \"Complaint ID\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "loan_complaint_data = loader.load()\n",
    "\n",
    "for doc in loan_complaint_data:\n",
    "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gQphb6y0C0S"
   },
   "source": [
    "Let's look at an example document to see if everything worked as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkUkCf7DaMiq",
    "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_complaint_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWaQpdHl0Gzc"
   },
   "source": [
    "## Task 3: Setting up QDrant!\n",
    "\n",
    "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
    "\n",
    "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
    "\n",
    "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NT8ihRJbYmMT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "small_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    loan_complaint_data,\n",
    "    small_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LoanComplaints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2SS4Rh0hiN"
   },
   "source": [
    "## Task 4: Naive RAG Chain\n",
    "\n",
    "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEH7X5Ai08FH"
   },
   "source": [
    "### R - Retrieval\n",
    "\n",
    "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
    "\n",
    "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GFDPrNBtb72o"
   },
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbBhyQjz06dx"
   },
   "source": [
    "### A - Augmented\n",
    "\n",
    "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7uSz-Dbqcoki"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlRzpb231GGJ"
   },
   "source": [
    "### G - Generation\n",
    "\n",
    "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "c-1t9H60dJLg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg3QRGzA1M2x"
   },
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "We're going to use LCEL to construct our chain.\n",
    "\n",
    "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0bvstS7mdOW3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.97 ms, sys: 3.47 ms, total: 10.4 ms\n",
      "Wall time: 47.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izKujhNb1ZG8"
   },
   "source": [
    "Let's see how this simple chain does on a few different prompts.\n",
    "\n",
    "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LI-5ueEddku9",
    "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 432 ms, sys: 19.2 ms, total: 451 ms\n",
      "Wall time: 2.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the most common issue with loans appears to be dealing with the lender or servicer, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and poor communication regarding loan transfers and account status. Many complaints also involve incorrect or outdated information on credit reports, inability to make proper payments or apply extra funds correctly, and issues related to loan mismanagement and lack of transparency.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "43zdcdUydtXh",
    "outputId": "db874e67-f568-4ed1-b863-b7c17b387052",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 401 ms, sys: 0 ns, total: 401 ms\n",
      "Wall time: 1.95 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, it appears that several complaints were related to delays or failures in handling issues in a timely manner. Specifically:\\n\\n- One complaint (row 441) from 03/28/25 regarding a loan application that showed no movement despite multiple follow-ups, was marked as \"Timely response? No.\"\\n- Another complaint (row 67) from 04/14/25 about a previous unresolved issue with auto pay and missing bank information was marked as \"Timely response? Yes,\" but the complaint indicates ongoing issues.\\n- Multiple complaints (rows 423, 518, 716, 810, 400, etc.) involve delays ranging from a few days to over a year, with some responses marked as \"Closed with explanation\" or \"None,\" and multiple instances of unresolved or delayed responses.\\n\\nTherefore, yes, there were complaints that did not get handled in a timely manner.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "lpG6rlvvvKFq",
    "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 403 ms, sys: 0 ns, total: 403 ms\n",
      "Wall time: 4.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"People failed to pay back their loans for several reasons, including:\\n\\n1. **Accumulation of interest and limited repayment options:** Many borrowers experienced ongoing interest accumulation, especially when loans were put into forbearance or deferment, which extended the repayment period and increased the total debt. Lowering monthly payments often resulted in interest compounding, making it difficult to pay down the principal.\\n\\n2. **Financial hardships and income challenges:** Borrowers faced economic difficulties such as stagnant wages, high living expenses, unemployment, or underemployment, which made it impossible to afford higher payments or even maintain existing payments.\\n\\n3. **Lack of clear communication and transparency:** Many borrowers were not adequately informed about their loan status, payment resumption dates, or changes in loan servicers. Sudden reporting of delinquency or late payments without proper notification led to credit score drops and financial setbacks.\\n\\n4. **Issues with loan management and servicing practices:** Some borrowers reported being placed on long-term forbearance, experiencing unresponsive or unhelpful customer service, or facing difficulties applying extra funds toward principal or paying off smaller loans faster.\\n\\n5. **Complicated and unfavorable repayment terms:** Borrowers often found repayment options limited or unaffordable and felt misled about the total amount owed, the impact of interest, or the availability of forgiveness programs.\\n\\n6. **Loan mismanagement and errors:** In some cases, there were errors such as incorrect reporting of delinquencies, improper transfer of loans between servicers without notification, or failure to notify borrowers when repayment was due, all of which contributed to missed payments.\\n\\nOverall, a combination of economic hardship, complex loan servicing practices, and poor communication contributed to many borrowers' inability to successfully repay their loans.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbfQmbr1leg"
   },
   "source": [
    "Overall, this is not bad! Let's see if we can make it better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft1vt8HPR16w"
   },
   "source": [
    "## Task 5: Best-Matching 25 (BM25) Retriever\n",
    "\n",
    "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
    "\n",
    "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
    "\n",
    "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "qdF4wuj5R-cG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIjJlBQ8drKH"
   },
   "source": [
    "We'll construct the same chain - only changing the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WR15EQG7SLuw"
   },
   "outputs": [],
   "source": [
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gi-yXCDdvJk"
   },
   "source": [
    "Let's look at the responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oY9qzmm3SOrF",
    "outputId": "4d4f450f-5978-460f-f242-b32407868353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 14.1 ms, total: 131 ms\n",
      "Wall time: 2.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, specifically issues such as:\\n\\n- Disputes over fees charged.\\n- Trouble with how payments are being handled, such as difficulty applying payments to the principal or paying off loans more quickly.\\n- Receiving inaccurate or bad information about loans, including loan balances, interest calculations, and loan history.\\n- Issues with loan terms, deferments, income-driven repayment plans, and loan duration.\\n\\nThese issues suggest that a prevalent problem is the lack of transparency, miscommunication, and difficulties in managing and understanding loan details. Therefore, the most common issue with loans, as reflected in the complaints, is **\"Dealing with lenders or servicers\" with specific problems like incorrect information, fee disputes, and payment application issues**.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "igfinyneSQkh",
    "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.4 ms, sys: 11.5 ms, total: 47.9 ms\n",
      "Wall time: 1.95 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, it appears that several complaints were handled with a response marked as \"Closed with explanation,\" and the responses were noted as \"Timely response?\": \"Yes\" in each case. However, some complaints involved ongoing issues where the complainants expressed dissatisfaction with the adequacy of the company\\'s response or unresolved issues, particularly regarding loan corrections or validation of debt, but the records indicate that responses were officially provided in a timely manner.\\n\\nTherefore, while formal responses were given promptly, the presence of unresolved or ongoing concerns in the complaints suggests that some issues may not have been fully addressed to the complainants\\' satisfaction in a timely manner. Nonetheless, based solely on the data about response timings, the complaints did receive timely responses, even if the issues remain unresolved.\\n\\nIn conclusion:  \\n**Yes, some complaints were answered promptly, but there are indications that certain complaints have not been resolved satisfactorily or completely in a timely manner.**'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "w0H7pV_USSMQ",
    "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.8 ms, sys: 17.9 ms, total: 62.7 ms\n",
      "Wall time: 1.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for various reasons, including problems with payment plans, miscommunication or lack of communication from the loan servicers, and issues related to incorrect or unauthorized transfer of their loans. Some specific causes include:\\n\\n- Being steered into wrong types of forbearances or experiencing improper handling of their repayment plans.\\n- Lack of response or failure to receive notices from loan servicers about important changes, such as loan transfers or due dates.\\n- Automatic payment issues, such as payments being unenrolled without proper notification or payments being reversed multiple times due to errors on the part of the servicers.\\n- Negative impact on credit scores due to billing errors or lack of contact from the servicers.\\n- Not being properly informed of their loan status or changes, leading to overdue payments and negative credit reporting.\\n- Attempts to seek hardship relief or forbearance not being addressed, resulting in continued billing and overdue amounts.\\n\\nOverall, failures to pay back loans often stem from administrative errors, poor communication, and inadequate customer service on the part of loan servicers, which lead to misunderstandings and missed payments by borrowers.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvg5xHaUdxCl"
   },
   "source": [
    "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question #1:\n",
    "\n",
    "Give an example query where BM25 is better than embeddings and justify your answer.\n",
    "\n",
    "#### ✅ Answer:\n",
    "\n",
    "BM25, a traditional full-text search ranking function, is particularly effective when dealing with queries that rely heavily on exact term matching, term frequency, and inverse document frequency (TF-IDF) principles.\n",
    "\n",
    "BM25 is generally better suited for scenarios where exact keyword matching is essential, such as in e-commerce search engines, document retrieval systems, and legal e-discovery.\n",
    "\n",
    "Additionally, BM25 is often used in hybrid search systems alongside vector search to create a more comprehensive understanding of both semantic meaning and keyword importance.\n",
    "\n",
    "Here are a couple of queries where the exact matching terms in the document would be essential to prevent a lot of results with noise and near close terms but not close enough:\n",
    "\n",
    "- \"Find documents about COVID-19 vaccine side effects in patients with diabetes\"\n",
    "  - the key terms here COVID-19 vaccine and diabetes are were the focus is in the query\n",
    "- \"Best practices for data backup in 2025\"\n",
    "  - It includes specific terms like \"data backup\" and \"2025\" that are likely to appear verbatim in relevant documents.\n",
    "  - BM25 can effectively leverage term frequency (e.g., how often \"data backup\" appears in a document) and document length normalization to rank documents accurately. The query does not heavily rely on semantic similarity but rather on the presence and frequency of exact keywords.\n",
    "  - In contrast, dense embeddings might struggle if the training data does not include similar phrasing or if the semantic model does not strongly associate \"best practices\" with \"data backup\" in the context of 2025.\n",
    "\n",
    "Embeddings, on the other hand, are better suited for capturing semantic relationships between words and documents. If embeddings were used in the above scenarios or use-cases, the precision of the results would not be as accurate as with BM25.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addendum\n",
    "\n",
    "_**Sparse Embeddings** are high-dimensional vectors where most values are zero, with only a few non-zero values representing specific features or tokens that are present, making them memory-efficient and interpretable but limited to explicit feature representation._\n",
    "\n",
    "_**Dense Embeddings** are vectors where most or all dimensions have non-zero values, creating rich, continuous representations that capture complex semantic relationships and contextual meaning, but require more storage and are less interpretable._\n",
    "\n",
    "_**Key Difference:** Sparse embeddings work like \"on/off switches\" for specific features (like one-hot encoding or TF-IDF), while dense embeddings work like \"semantic fingerprints\" where every dimension contributes to the overall meaning representation - sparse focuses on explicit presence/absence, dense captures nuanced relationships._\n",
    "\n",
    "___\n",
    "\n",
    "_**Sparse Retrieval** uses exact keyword matching with algorithms like BM25, where documents are represented as sparse vectors containing only the specific terms that appear in them, making it excellent for precise term-based searches but limited to lexical matches._\n",
    "\n",
    "_**Dense Retrieval** uses semantic embeddings where documents and queries are converted into dense vector representations that capture meaning and context, allowing it to find semantically similar content even when different words are used, but potentially missing exact keyword matches._\n",
    "\n",
    "_**Key Difference:** Sparse retrieval excels at \"what you search is what you get\" with exact terms, while dense retrieval excels at \"what you mean is what you get\" through semantic understanding - which is why hybrid approaches combining both often work best._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-dcbFn2vpZF"
   },
   "source": [
    "## Task 6: Contextual Compression (Using Reranking)\n",
    "\n",
    "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
    "\n",
    "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
    "\n",
    "The basic idea here is this:\n",
    "\n",
    "- We retrieve lots of documents that are very likely related to our query vector\n",
    "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
    "\n",
    "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
    "\n",
    "All we need to do is the following:\n",
    "\n",
    "- Create a basic retriever\n",
    "- Create a compressor (reranker, in this case)\n",
    "\n",
    "That's it!\n",
    "\n",
    "Let's see it in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "psHvO2K1v_ZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TA9RB2x-j7P"
   },
   "source": [
    "Let's create our chain again, and see how this does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1BXqmxvHwX6T"
   },
   "outputs": [],
   "source": [
    "contextual_compression_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V3iGpokswcBb",
    "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 429 ms, sys: 0 ns, total: 429 ms\n",
      "Wall time: 1.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the most common issue with loans appears to be problems related to the handling and management of the loans by servicers, including errors in loan balances, misapplied payments, incorrect or bad information, and issues with communication and documentation. Specifically, issues such as errors in loan balances, misapplied payments, wrongful denials of payment plans, and mishandling of information are frequently mentioned.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "7u_k0i4OweUd",
    "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 425 ms, sys: 0 ns, total: 425 ms\n",
      "Wall time: 1.84 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, according to the provided complaints, at least one complaint was not handled in a timely manner. Specifically, the complaint regarding the student loan issues submitted to Maximus Federal Services, Inc. has been open for over 1 year and nearly 18 months without resolution. The complaint from EdFinancial Services about payments not being applied also indicates ongoing issues, although the response was marked as \"closed with explanation.\"'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "zn1EqaGqweXN",
    "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 405 ms, sys: 0 ns, total: 405 ms\n",
      "Wall time: 2.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans primarily due to a lack of clear communication and understanding about their loan obligations, as well as issues related to the management and handling of their loans by servicers. Specifically, some borrowers were unaware that they needed to repay their loans, and they were not adequately informed about the details of interest accumulation, payment options, or changes in loan ownership. Additionally, difficulties such as technical problems accessing online accounts, lack of notifications about repayment requirements, and complex or conflicting loan account information contributed to borrowers being unable to fulfill their repayment obligations. In some cases, borrowers also experienced financial hardship because the available repayment options, such as forbearance or deferment, led to ongoing interest accumulation, making it even harder to pay off the loans over time.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEbT0g2S-mZ4"
   },
   "source": [
    "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqbghrBEQNn5"
   },
   "source": [
    "## Task 7: Multi-Query Retriever\n",
    "\n",
    "Typically in RAG we have a single query - the one provided by the user.\n",
    "\n",
    "What if we had....more than one query!\n",
    "\n",
    "In essence, a Multi-Query Retriever works by:\n",
    "\n",
    "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
    "2. Retrieving documents for each query.\n",
    "3. Using all unique retrieved documents as context\n",
    "\n",
    "So, how is it to set-up? Not bad! Let's see it down below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "pfM26ReXQjzU"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "1vRc129jQ5WW"
   },
   "outputs": [],
   "source": [
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "CGgNuOb3Q3M9",
    "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 0 ns, total: 1.15 s\n",
      "Wall time: 3.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The most common issues with loans, based on the complaints provided, are:\\n\\n- Dealing with lenders or servicers, including trouble with how payments are handled, misapplication of payments, and lack of communication.\\n- Problems with loan balances, interest calculations, and errors in account information.\\n- Issues related to loan management practices such as forbearance steering, unauthorized interest capitalization, and improper loan deferments.\\n- Problems with loan forgiveness, cancellation, or discharge, often involving mismanagement, missed opportunities, or fraud concerns.\\n- Difficulties in obtaining loan information, account validation, and transparency about terms and balances.\\n- Errors leading to negative impacts on credit reports and scores.\\n\\nOverall, the most common underlying theme appears to be **mismanagement or poor communication by loan servicers**, leading to errors in balances, interest, and account status, which significantly affect borrowers' financial health and stability.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aAlSthxrRDBC",
    "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 0 ns, total: 1.15 s\n",
      "Wall time: 4.38 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, yes, some complaints did not get handled in a timely manner. Specifically, there are complaints from consumers indicating delays in responses or resolution, such as:\\n\\n- Complaint with ID 12709087 (submitted 03/28/25) from CA: The company response was \"Closed with explanation,\" and it was marked as \"Not timely.\" The consumer reported that it has been nearly 18 months with no resolution.\\n- Complaint with ID 12739706 (submitted 04/01/25) from NJ: Marked as \"Not timely.\"\\n- Complaint with ID 12668396 (submitted 03/26/25) from NJ: Marked as \"Not timely.\"\\n- Complaint with ID 13056764 (submitted 04/18/25) from IN: Marked as \"Timely,\" but involves ongoing issues with inaction.\\n- Multiple other complaints show delays, failed follow-ups, or responses with explanations that indicate unresolved issues over long periods.\\n\\nWhile some complaints were responded to promptly, others experienced significant delays or lack of resolution, indicating that not all complaints received timely handling.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Uv1mpCK8REs4",
    "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 0 ns, total: 1.18 s\n",
      "Wall time: 5.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"People failed to pay back their loans primarily due to systemic issues and misconduct by loan servicers and lenders. The provided complaints reveal several common reasons:\\n\\n1. **Mismanagement and Errors by Servicers:** Many complain about errors in loan balances, misapplied payments, incorrect account statuses, and improper transfer of accounts between servicers without proper notification or authorization. For example, some borrowers' accounts were reported as delinquent or in default despite being current, often due to reporting errors or lack of proper notices.\\n\\n2. **Lack of Proper Communication and Notification:** Several accounts of borrowers not being informed about the resumption of payments, changes in servicers, or delinquency status. For instance, borrowers reported receiving no prior notices before being marked as late or delinquent, which hindered their ability to manage payments.\\n\\n3. **Inadequate Guidance and Support:** Complaints include instances where servicers failed to inform borrowers about available repayment options such as income-driven repayment plans or loan rehabilitation, leading borrowers into long-term forbearances. These long forbearances often resulted in increased interest and ballooned balances.\\n\\n4. **Aggressive and Coercive Practices:** Reports of forbearance steering, where borrowers were pressured into forbearances or consolidations without proper disclosure of alternatives, ultimately increasing their debt burden and interest capitalization. Some borrowers were coerced into consolidations under false pretenses, causing balances to increase significantly.\\n\\n5. **Errors and Fraudulent Activities:** There are complaints about accounts being accessed without permission, inaccurate reporting, and even potential FERPA violations, which can lead to confusion and default misreporting.\\n\\n6. **Financial Hardship and Lack of Alternatives:** Many borrowers faced financial hardship, unemployment, or medical issues, which made repayment difficult, especially when servicers failed to offer or inform borrowers about alternative programs like income-driven plans, loan forgiveness, or discharge options.\\n\\nIn summary, the failure to repay loans often stemmed from systemic mismanagement, lack of transparency, poor communication, coercive practices by servicers, and mishandling of accounts, all of which impeded borrowers’ ability to manage their loans effectively.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question #2:\n",
    "\n",
    "Explain how generating multiple reformulations of a user query can improve recall.\n",
    "\n",
    "#### ✅ Answer:\n",
    "\n",
    "Multiple reformulations improve recall because relevant documents may use different terminology than the original query, and each reformulation can surface documents the others miss (different phrasings in multiple reformulations of a query can match different relevant documents).\n",
    "\n",
    "In other words, multiple reformulations approach the same query from different angles/facets, leading to retrieval of documents covering those various angles. This increases the confluence of documents around the common theme while capturing variations in terminology and perspective, thereby enhancing retrieval scope.\n",
    "\n",
    "And since such retrievers that use multiple reformulations would follow the below steps:\n",
    "\n",
    "  1. Generates multiple query variations from the original query using an LLM\n",
    "  2. Retrieves documents for each variation (each gets k results)\n",
    "  3. Deduplicates and merges the results from all queries\n",
    "  4. Returns the final deduplicated set\n",
    "\n",
    "The return results from multiple reformulations would be more beneficial as a retrieval process.\n",
    "\n",
    "An example would be \"machine learning algorithms\" vs \"AI models\" retrieves different relevant documents but around the same or similar theme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDEawBf_d_3G"
   },
   "source": [
    "## Task 8: Parent Document Retriever\n",
    "\n",
    "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
    "\n",
    "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
    "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
    "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
    "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
    "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
    "\n",
    "Okay, maybe that was a few steps - but the basic idea is this:\n",
    "\n",
    "- Search for small documents\n",
    "- Return big documents\n",
    "\n",
    "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
    "\n",
    "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qJ53JJuMd_ZH"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "parent_docs = loan_complaint_data\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpXfVUH3gL3"
   },
   "source": [
    "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
    "\n",
    "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzFc-_9HlGQ-",
    "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20169/921369447.py:6: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
      "  parent_document_vectorstore = Qdrant(\n"
     ]
    }
   ],
   "source": [
    "vectorstore.client.create_collection(\n",
    "  collection_name=\"full_documents\",\n",
    "  vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = Qdrant(\n",
    "  client=vectorstore.client,     # ✅ Reuse existing client\n",
    "  embeddings=small_embeddings,         # ✅ Reuse embeddings\n",
    "  collection_name=\"full_documents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_g95FA3s6w"
   },
   "source": [
    "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "BpWVjPf4fLUp"
   },
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoYmSWfE32Zo"
   },
   "source": [
    "By default, this is empty as we haven't added any documents - let's add some now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "iQ2ZzfKigMZc"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI7Tip1335rE"
   },
   "source": [
    "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Qq_adt2KlSqp"
   },
   "outputs": [],
   "source": [
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNolUVQb4Apt"
   },
   "source": [
    "Let's give it a whirl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "TXB5i89Zly5W",
    "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 394 ms, sys: 21.2 ms, total: 415 ms\n",
      "Wall time: 1.21 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The most common issue with loans, based on the provided complaints, appears to be related to mismanagement and errors by loan servicers. This includes problems such as incorrect information on credit reports, discrepancies in loan balances and interest rates, wrongful denials of payment plans, errors in loan accounting, and issues stemming from the transfer or sale of loans. Many complaints highlight systemic breakdowns, misapplication of payments, misleading reporting, and difficulty in resolving discrepancies.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V5F1T-wNl3cg",
    "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 398 ms, sys: 0 ns, total: 398 ms\n",
      "Wall time: 1.64 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, several complaints explicitly mention that they were not handled in a timely manner. For example, the complaint about the student loan application filed on 03/28/25 indicates that the consumer waited for responses and had not heard back, with responses delayed beyond the estimated timeframes. Similarly, complaints filed on 04/11/25 regarding issues with Mohela's processing times also note that responses were not timely. The complaint about the dispute settlement sent over 30 days ago also emphasizes that the issue has not been addressed within a reasonable period.\\n\\nTherefore, yes, there are complaints within the provided data that did not get handled in a timely manner.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ZqARszGzvGcG",
    "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 395 ms, sys: 0 ns, total: 395 ms\n",
      "Wall time: 1.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'People may fail to pay back their loans due to various reasons, including financial hardship, lack of proper information or communication from lenders, misrepresentation by educational institutions about the value or stability of their programs, and systemic issues within loan servicing agencies. Specifically, some individuals face difficulties because they were not properly informed about repayment obligations, experienced severe financial hardship after graduation, or encountered issues with loan servicing companies that failed to communicate correctly or manage their accounts transparently.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B41cj42s4DPM"
   },
   "source": [
    "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUrIBKl_TwS9"
   },
   "source": [
    "## Task 9: Ensemble Retriever\n",
    "\n",
    "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
    "\n",
    "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
    "\n",
    "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "8j7jpZsKTxic"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpo9Psl5hhJ-"
   },
   "source": [
    "We'll pack *all* of these retrievers together in an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "KZ__EZwpUKkd"
   },
   "outputs": [],
   "source": [
    "ensemble_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSsvHpRMj24L"
   },
   "source": [
    "Let's look at our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lMvqL88UQI-",
    "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.25 s, sys: 0 ns, total: 2.25 s\n",
      "Wall time: 4.74 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The most common issue with loans, based on the complaints in the provided data, appears to be problems related to how payments are being handled, mismanagement by servicers, and errors in loan information. Specifically, recurring issues include:\\n\\n- Payments being reversed without explanation, causing credit scores to drop and inaccurate reporting of delinquency.\\n- Errors in reporting loan status, such as incorrect delinquency or late payment marks.\\n- Lack of clear communication or notification about account changes, transfers, or balances.\\n- Discrepancies and inaccuracies in loan balances and payment histories.\\n- Failure of servicers to properly process payments or update account information.\\n- Issues with loan classification, mismanagement, and mishandling of deferments or forbearance.\\n\\nWhile other problems like bad information, incorrect balances, or dispute of validity also occur, the most frequent pattern is the mishandling of payments and inaccurate reporting by loan servicers, leading to credit damage and confusion for borrowers.\\n\\nIf you need a concise summary:  \\n**The most common issue with loans is mishandling of payments and inaccurate reporting of loan status by servicers.**'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "MNFWLYECURI1",
    "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 0 ns, total: 2.27 s\n",
      "Wall time: 6.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints data, yes, some complaints indicate that issues were not handled in a timely manner. Specifically:\\n\\n- Complaint ID 12935889 (Page 1): The response was marked as \"No\" for timely response, and the response was \"Closed with explanation.\"\\n- Complaint ID 12739706 (Page 2): The response was \"Closed with explanation,\" and it was marked as \"No\" for timely response.\\n- Complaint ID 12950199 (Page 2): The response was \"Closed with explanation,\" but it was marked as \"Yes\" for timely response.\\n- Complaint ID 12973003 (Page 2): The response was \"Closed with explanation,\" and it was marked as \"Yes\" for timely response.\\n- Complaint ID 13062402 (Page 2): The response was \"Closed with explanation,\" and it was marked as \"Yes\" for timely response.\\n- Several other complaints, such as IDs 13160766, 13205525, and 13197090, are marked as timely, but some still resulted in closure with explanation, which may indicate handling was not fully satisfactory or timely.\\n\\nIn summary, there are instances where complaints were not handled in a timely manner or were closed without resolution.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "A7qbHfWgUR4c",
    "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 0 ns, total: 2.27 s\n",
      "Wall time: 7.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"People failed to pay back their loans primarily due to a combination of factors highlighted in the complaints:\\n\\n1. **Lack of clear communication and notification:** Many borrowers were not adequately informed about their repayment status, whether their loans had resumed, or when payments were due. Several complaints mention not receiving proper notices, emails, or mail, leading to unexpected delinquencies and credit impacts.\\n\\n2. **Mismanagement and errors by loan servicers:** Servicers, such as Nelnet, Maximus, EdFinancial, and others, have been reported to mishandle accounts—incorrectly reporting delinquencies, refusing to provide accurate information, or failing to follow regulations. This mismanagement often results in increased balances, erroneous defaults, or inaccurate credit reporting.\\n\\n3. **Interest accumulation and forbearance practices:** Borrowers were often steered into forbearance or deferment without adequate understanding of how interest would accrue or options to minimize long-term costs. Long-term forbearance can cause interest to compound, increasing debt beyond the original loan amount.\\n\\n4. **Inability to access or understand repayment options:** Many complainants did not receive information about income-driven repayment plans, loan forgiveness programs, or the consequences of forbearance and consolidation. Without this guidance, borrowers could inadvertently fall into default or see their balances grow due to accumulated interest.\\n\\n5. **Payment processing issues:** Some borrowers experienced payments being reversed or not correctly applied, leading to delinquencies and negative credit impacts, despite making timely payments.\\n\\n6. **Loan transfers and improper handling:** Multiple complaints mention loans being transferred or sold without proper notification, causing confusion and errors in account management.\\n\\n7. **Economic hardship and personal circumstances:** A subset of borrowers faced financial hardship, unemployment, or health issues, making repayment unsustainable, especially when combined with high interest, miscommunication, and lack of supportive options.\\n\\nOverall, failure to repay was often not due to irresponsibility but rather systemic mismanagement, lack of transparent communication, and practices that hindered borrowers' ability to manage or understand their loans effectively.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1533"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MopbkNJAXVaN"
   },
   "source": [
    "## Task 10: Semantic Chunking\n",
    "\n",
    "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
    "\n",
    "Essentially, Semantic Chunking is implemented by:\n",
    "\n",
    "1. Embedding all sentences in the corpus.\n",
    "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
    "  - `percentile`\n",
    "  - `standard_deviation`\n",
    "  - `interquartile`\n",
    "  - `gradient`\n",
    "3. Each sequence of related sentences is kept as a document!\n",
    "\n",
    "Let's see how to implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `breakpoint_threshold_type` parameter controls when the semantic chunker creates chunk boundaries based on embedding similarity between sentences:\n",
    "\n",
    "**Four Threshold Types:**\n",
    "\n",
    "1. _\"percentile\" (default)_\n",
    "- Splits when sentence embedding distance exceeds the 95th percentile of all distances\n",
    "- Effect: Creates chunks at the most semantically distinct boundaries\n",
    "- Behavior: More conservative splitting, larger chunks\n",
    "\n",
    "2. _\"standard_deviation\"_\n",
    "- Splits when distance exceeds 3 standard deviations from mean\n",
    "- Effect: Better predictable performance, especially for normally distributed content\n",
    "- Behavior: More consistent chunk sizes\n",
    "\n",
    "3. _\"interquartile\"_\n",
    "- Uses IQR * 1.5 scaling factor to determine breakpoints\n",
    "- Effect: Middle-ground approach, robust to outliers\n",
    "- Behavior: Balanced chunk distribution\n",
    "\n",
    "4. _\"gradient\"_\n",
    "- Detects anomalies in embedding distance gradients\n",
    "- Effect: Best for domain-specific/highly correlated content\n",
    "- Behavior: Finds subtle semantic transitions\n",
    "\n",
    "**Impact:** _The threshold type determines sensitivity to semantic changes - more sensitive types create smaller, more focused chunks while less sensitive types create larger, more comprehensive chunks._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ciZbFEldv_"
   },
   "source": [
    "We'll use the `percentile` thresholding method for this example which will:\n",
    "\n",
    "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "66EIEWiEYl5y"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    small_embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqoKmz12mhRW"
   },
   "source": [
    "Now we can split our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ROcV7o68ZIq7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 304 ms, sys: 0 ns, total: 304 ms\n",
      "Wall time: 8.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-LNC-Xmjex"
   },
   "source": [
    "Let's create a new vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "h3sl9QjyZhIe"
   },
   "outputs": [],
   "source": [
    "vectorstore.client.create_collection(\n",
    "  collection_name=\"Loan_Complaint_Data_Semantic_Chunks\",\n",
    "  vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "semantic_vectorstore = Qdrant(\n",
    "  client=vectorstore.client,     # ✅ Reuse existing client\n",
    "  embeddings=small_embeddings,         # ✅ Reuse embeddings\n",
    "  collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
    ")\n",
    "\n",
    "# Add documents after creation\n",
    "_ = semantic_vectorstore.add_documents(semantic_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh_r_-LHmmKn"
   },
   "source": [
    "We'll use naive retrieval for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "odVyDUHwZftc"
   },
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkeiv_ojmp6G"
   },
   "source": [
    "Finally we can create our classic chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "xWE_0J0mZveG"
   },
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5pfjLQ3ms9_"
   },
   "source": [
    "And view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lN2j-e4Z0SD",
    "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common issue with loans, based on the provided complaints, appears to be problems related to loan servicing and communication issues. These include difficulties with repayment, errors or delays in payment processing, issues with loan reporting and credit impact, and lack of clear information or transparency from loan servicers. Many complainants report being unable to get accurate or timely information about their loan status, repayment amounts, or servicing changes.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "xdqfBH1SZ3f9",
    "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, several complaints indicate that they were handled in a timely manner, as the responses from companies were marked as \"Closed with explanation\" and responded to within the expected time frames. For example, complaints received on 04/28/25, 05/01/25, 05/04/25, and 05/09/25 all state that responses were timely. \\n\\nHowever, the first complaint from 05/04/25 involving Nelnet mentions that despite acknowledgment of receipt and response, the complaint involved serious unresolved issues such as unresponded to certified mail and ongoing violations—suggesting that some aspects of the complaint were not fully handled or resolved to the complainant\\'s satisfaction.\\n\\nGiven the available data, there is no explicit mention of complaints that were not handled in a timely manner overall. Most responses are marked \"Yes\" for timely response. Nonetheless, some complaints involve ongoing issues or unresolved problems despite responses being sent.\\n\\nTherefore, based on the provided information, I do not see clear evidence that any complaints were definitively *not* handled in a timely manner.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "rAcAObZnZ4o6",
    "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"People failed to pay back their loans mainly due to issues such as mismanagement or lack of proper communication from lenders or servicers, disputes over the legitimacy of the debt, technical problems with payment processing, and inadequate or delayed re-amortization of payments after forbearance periods. Additionally, some individuals faced complications related to incorrect or disputed information on their credit reports, and there are reports of alleged improper or illegal reporting and collection practices by loan servicers. These factors can create barriers or uncertainties that hinder borrowers' ability or willingness to repay their loans promptly.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1405"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ❓ Question #3:\n",
    "\n",
    "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
    "\n",
    "#### ✅ Answer:\n",
    "\n",
    "Short and highly repetitive sentences create _minimal embedding distance_ variations, making it difficult to detect _meaningful semantic_ boundaries.\n",
    "\n",
    "Threshold Type Behaviors:\n",
    "\n",
    "1. \"percentile\" (95th percentile)\n",
    "\n",
    "- Behavior: Creates very few chunks since most distances are similar\n",
    "- Issue: May group unrelated FAQ topics together\n",
    "- Adjustment: Lower to 75-85th percentile to increase sensitivity\n",
    "\n",
    "2. \"standard_deviation\" (3σ)\n",
    "\n",
    "- Behavior: Performs poorly due to low variance in short, similar sentences\n",
    "- Issue: Creates massive chunks with no meaningful breaks\n",
    "- Adjustment: Reduce to 1-2 standard deviations for more splitting\n",
    "\n",
    "3. \"interquartile\" (IQR × 1.5)\n",
    "\n",
    "- Behavior: Most robust for FAQs due to outlier resistance\n",
    "- Issue: Still may miss subtle topic transitions\n",
    "- Adjustment: Reduce scaling factor to 0.8-1.0\n",
    "\n",
    "4. \"gradient\" (anomaly detection)\n",
    "\n",
    "- Behavior: Best performer - detects subtle topic shifts in repetitive content\n",
    "- Issue: May be overly sensitive to minor variations\n",
    "- Adjustment: Fine-tune threshold to 85-90th percentile\n",
    "\n",
    "Conclusion: Use \"gradient\" with _85th percentile_ + minimum chunk size constraints + keyword-based post-processing to ensure FAQ topics remain grouped appropriately despite repetitive language patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk2n3-pnVWDJ"
   },
   "source": [
    "# 🤝 Breakout Room Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SkJLYwMVZkj"
   },
   "source": [
    "#### 🏗️ Activity #1\n",
    "\n",
    "Your task is to evaluate the various Retriever methods against eachother.\n",
    "\n",
    "You are expected to:\n",
    "\n",
    "1. Create a \"golden dataset\"\n",
    " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
    "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
    " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
    "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
    "\n",
    "Your analysis should factor in:\n",
    "  - Cost\n",
    "  - Latency\n",
    "  - Performance\n",
    "\n",
    "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWAr16a5XMub"
   },
   "source": [
    "##### HINTS:\n",
    "\n",
    "- LangSmith provides detailed information about latency and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "tgDICngKXLGK"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_env_var_is_set(env_var_name: str, human_readable_string: str = \"API Key\"):\n",
    "    api_key = os.getenv(env_var_name)\n",
    "  \n",
    "    if api_key:\n",
    "       print(f\"{env_var_name} is present\")\n",
    "    else:\n",
    "      print(f\"{env_var_name} is NOT present, paste key at the prompt:\")\n",
    "      os.environ[env_var_name] = getpass.getpass(f\"Please enter your {human_readable_string}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGCHAIN_API_KEY is present\n",
      "LANGSMITH_API_KEY is present\n",
      "OPENAI_API_KEY is present\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "check_if_env_var_is_set(\"LANGCHAIN_API_KEY\", \"LangChain API key\")\n",
    "check_if_env_var_is_set(\"LANGSMITH_API_KEY\", \"LangSmith API key\")\n",
    "check_if_env_var_is_set(\"OPENAI_API_KEY\", \"OpenAI API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - ADwLC - {uuid4().hex[0:8]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langsmith_project_name = os.environ[\"LANGCHAIN_PROJECT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "path = \"data/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 0, relationships: 0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 0, relationships: 0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset.graph import Node, NodeType\n",
    "if not os.path.exists('loan_data_kg.json'):\n",
    "    ### NOTICE: We're using a subset of the data for this example - this is to keep costs/time down.\n",
    "    for doc in docs[:10]: ### 20\n",
    "        kg.nodes.append(\n",
    "            Node(\n",
    "                type=NodeType.DOCUMENT,\n",
    "                properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "            )\n",
    "        )\n",
    "kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.52 s, sys: 721 ms, total: 2.24 s\n",
      "Wall time: 2.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 0, relationships: 0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "transformer_llm = generator_llm\n",
    "embedding_model = generator_embeddings\n",
    "\n",
    "if not os.path.exists('loan_data_kg.json'):\n",
    "    default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
    "    apply_transforms(kg, default_transforms)\n",
    "else:\n",
    "    kg.load('loan_data_kg.json')\n",
    "kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 175 ms, total: 1.5 s\n",
      "Wall time: 1.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 43, relationships: 647)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists('loan_data_kg.json'):\n",
    "    kg.save(\"loan_data_kg.json\")\n",
    "    \n",
    "loan_data_kg = KnowledgeGraph.load(\"loan_data_kg.json\")\n",
    "loan_data_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before generation: 686.7 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Check memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Memory usage before generation: {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=loan_data_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
    "\n",
    "query_distribution = [\n",
    "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
    "        (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
    "        (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66 μs, sys: 0 ns, total: 66 μs\n",
      "Wall time: 71.3 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testset = None\n",
    "if not os.path.exists('golden-master.csv'):\n",
    "    testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
    "    testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after generation: 686.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Check memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Memory usage after generation: {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after gc.collect(): 686.7 MB\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Memory usage after gc.collect(): {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the role of the School Participation D...</td>\n",
       "      <td>['Chapter 1 Academic Years, Academic Calendars...</td>\n",
       "      <td>The context does not specify the exact role of...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does 34 CFR 668.3(a) specify regarding th...</td>\n",
       "      <td>['Regulatory Citations Academic year minimums:...</td>\n",
       "      <td>34 CFR 668.3(a) pertains to the minimum number...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the context of Chapter 3, how does the incl...</td>\n",
       "      <td>['Inclusion of Clinical Work in a Standard Ter...</td>\n",
       "      <td>Inclusion of clinical work in a standard term ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you explain how Title IV regulations apply...</td>\n",
       "      <td>['Non-Term Characteristics A program that meas...</td>\n",
       "      <td>Payment periods under Title IV are applicable ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How are academic calendars defined for differe...</td>\n",
       "      <td>['Chapter 1 Academic Years, Academic Calendars...</td>\n",
       "      <td>Academic calendars are defined for each progra...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wht are the regultory citatons for academic ye...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nChapter 1 Academic Years, Academi...</td>\n",
       "      <td>The regulatory citations for academic years an...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Include clinical work in standard term periods...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nInclusion of Clinical Work in a S...</td>\n",
       "      <td>The context explains that clinical work conduc...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does the impact of term length and measure...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nInclusion of Clinical Work in a S...</td>\n",
       "      <td>The inclusion of clinical work in standard ter...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do Chapters 2 and 3 collectively address t...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nChapter 1 Academic Years, Academi...</td>\n",
       "      <td>Chapter 2 details the requirements for establi...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do different academic years for programs a...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nChapter 1 Academic Years, Academi...</td>\n",
       "      <td>Different academic years for various programs ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does Volume 8 address inclusion of clinica...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nInclusion of Clinical Work in a S...</td>\n",
       "      <td>Volume 8 explains that clinical work conducted...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What is the role of the School Participation D...   \n",
       "1   What does 34 CFR 668.3(a) specify regarding th...   \n",
       "2   In the context of Chapter 3, how does the incl...   \n",
       "3   Can you explain how Title IV regulations apply...   \n",
       "4   How are academic calendars defined for differe...   \n",
       "5   Wht are the regultory citatons for academic ye...   \n",
       "6   Include clinical work in standard term periods...   \n",
       "7   How does the impact of term length and measure...   \n",
       "8   How do Chapters 2 and 3 collectively address t...   \n",
       "9   How do different academic years for programs a...   \n",
       "10  How does Volume 8 address inclusion of clinica...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   ['Chapter 1 Academic Years, Academic Calendars...   \n",
       "1   ['Regulatory Citations Academic year minimums:...   \n",
       "2   ['Inclusion of Clinical Work in a Standard Ter...   \n",
       "3   ['Non-Term Characteristics A program that meas...   \n",
       "4   ['Chapter 1 Academic Years, Academic Calendars...   \n",
       "5   ['<1-hop>\\n\\nChapter 1 Academic Years, Academi...   \n",
       "6   ['<1-hop>\\n\\nInclusion of Clinical Work in a S...   \n",
       "7   ['<1-hop>\\n\\nInclusion of Clinical Work in a S...   \n",
       "8   ['<1-hop>\\n\\nChapter 1 Academic Years, Academi...   \n",
       "9   ['<1-hop>\\n\\nChapter 1 Academic Years, Academi...   \n",
       "10  ['<1-hop>\\n\\nInclusion of Clinical Work in a S...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   The context does not specify the exact role of...   \n",
       "1   34 CFR 668.3(a) pertains to the minimum number...   \n",
       "2   Inclusion of clinical work in a standard term ...   \n",
       "3   Payment periods under Title IV are applicable ...   \n",
       "4   Academic calendars are defined for each progra...   \n",
       "5   The regulatory citations for academic years an...   \n",
       "6   The context explains that clinical work conduc...   \n",
       "7   The inclusion of clinical work in standard ter...   \n",
       "8   Chapter 2 details the requirements for establi...   \n",
       "9   Different academic years for various programs ...   \n",
       "10  Volume 8 explains that clinical work conducted...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   single_hop_specifc_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if testset:\n",
    "    testset_df = testset.to_pandas()\n",
    "    testset_df.to_csv('golden-master.csv', index=False)\n",
    "else:\n",
    "    testset_df = pd.read_csv('golden-master.csv')\n",
    "testset_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing dataset: Loan Synthetic Data (s09)\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "langsmith_client = Client(\n",
    "    timeout_ms=60000,  # 60 seconds\n",
    "    retry_config={\"max_retries\": 5}\n",
    ")\n",
    "\n",
    "dataset_name = \"Loan Synthetic Data (s09)\"\n",
    "\n",
    "existing_datasets = langsmith_client.list_datasets()\n",
    "dataset_exists = any(dataset.name == dataset_name for dataset in existing_datasets)\n",
    "\n",
    "if dataset_exists:\n",
    "  langsmith_dataset = langsmith_client.read_dataset(dataset_name=dataset_name)\n",
    "  print(f\"Using existing dataset: {dataset_name}\")\n",
    "else:\n",
    "  langsmith_dataset = langsmith_client.create_dataset(\n",
    "      dataset_name=dataset_name,\n",
    "      description=\"Loan Synthetic Data (for s09 exercise)\"\n",
    "  )\n",
    "  print(f\"Created new dataset: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_row in testset_df.iterrows():\n",
    "  langsmith_client.create_example(\n",
    "      inputs={\n",
    "          \"question\": data_row[1][\"user_input\"]\n",
    "      },\n",
    "      outputs={\n",
    "          \"answer\": data_row[1][\"reference\"]\n",
    "      },\n",
    "      metadata={\n",
    "          \"context\": data_row[1][\"reference_contexts\"]\n",
    "      },\n",
    "      dataset_id=langsmith_dataset.id\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "rag_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "small_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "try:\n",
    "    vectorstore.client.create_collection(\n",
    "      collection_name=\"Loan RAG (semantic)\",\n",
    "      vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "semantic_vectorstore = Qdrant(\n",
    "    client=vectorstore.client,     # ✅ Reuse existing client\n",
    "    embeddings=small_embeddings,         # ✅ Reuse embeddings\n",
    "    collection_name=\"Loan RAG (semantic)\"\n",
    ")\n",
    "\n",
    "_ = semantic_vectorstore.add_documents(rag_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and question, you must answer the question based only on context.\n",
    "\n",
    "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith Evaluation Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = ChatOpenAI(model=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\n",
    "    \"qa\",\n",
    "    config={\"llm\": eval_llm},\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"response\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")  \n",
    "\n",
    "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"helpfulness\": (\n",
    "                \"Is this submission helpful to the user,\"\n",
    "                \" taking into account the correct reference answer?\"\n",
    "            )\n",
    "        },\n",
    "        \"llm\" : eval_llm\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"response\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "empathy_evaluator = LangChainStringEvaluator(\n",
    "    \"criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"empathy\": \"Is this response empathetic? Does it make the user feel like they are being heard?\",\n",
    "        },\n",
    "        \"llm\": eval_llm \n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "       \"prediction\": run.outputs[\"response\"],\n",
    "       \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dope-ifying Our Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPATHY_RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and question, you must answer the question based only on context.\n",
    "\n",
    "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
    "\n",
    "You must answer the question using empathy and kindness, and make sure the user feels heard.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "empathy_rag_prompt = ChatPromptTemplate.from_template(EMPATHY_RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "rag_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "large_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vectorstore.client.create_collection(\n",
    "      collection_name=\"Loan Data for RAG\",\n",
    "      vectors_config=models.VectorParams(size=3072, distance=models.Distance.COSINE) ### was 1536\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dope_app_vectorstore = Qdrant(\n",
    "  client=vectorstore.client,     # ✅ Reuse existing client\n",
    "  embeddings=large_embeddings,         # ✅ Reuse embeddings\n",
    "  collection_name=\"Loan Data for RAG\"\n",
    ")\n",
    "\n",
    "# Add documents after creation\n",
    "_ = dope_app_vectorstore.add_documents(rag_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "empathy_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | empathy_rag_prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_stages_folder_name = \".pipeline-stages\"\n",
    "os.makedirs(pipeline_stages_folder_name, exist_ok=True)\n",
    "def write_to_file(filename: str, content: str):\n",
    "    with open(f\"{pipeline_stages_folder_name}/{filename}\", 'w') as text_file:\n",
    "        try:\n",
    "            text_file.write(content)\n",
    "        finally:\n",
    "            text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Performance Analysis\n",
    "\n",
    "Now that we have evaluation data from LangSmith, let's analyze the performance of different retrievers across multiple dimensions: **Performance**, **Cost**, and **Latency**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Quick Analysis (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the performance analysis script\n",
    "# from performance_analysis import RetrieverPerformanceAnalyzer, analyze_retriever_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Final Solution - Dataset-Based Analysis\n",
    "\n",
    "Based on the working LangSmith pattern, this approach uses the dataset name to find evaluation sessions and extract runs properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting retriever performance analysis...\n",
      "📊 Dataset: Loan Synthetic Data (s09)\n",
      "📅 Looking back: 7 days\n",
      "--------------------------------------------------\n",
      "=== RETRIEVER PERFORMANCE ANALYSIS ===\n",
      "Getting evaluation runs for dataset: Loan Synthetic Data (s09)\n",
      "Dataset approach failed: Failed to POST /runs/query in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/query', '{\"detail\":\"At least one of \\'session\\', \\'id\\', \\'parent_run\\', \\'trace\\' or \\'reference_example\\' must be specified\"}')\n",
      "Fallback: Getting all recent runs... (this may take a bit of time)\n",
      "Filtered to 1 projects matching prefix 'AIM - ADwLC - 2b1903b4'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa77201aa9b4f2182d29353ddfea151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fallback found 273 total runs\n",
      "Analyzing runs: 273 runs\n",
      "  -> Processed 273 runs, avg_cost: $0.0002, avg_latency: 2.11s\n",
      "\n",
      "❌ Error during analysis: object of type 'NoneType' has no len()\n",
      "Error type: <class 'TypeError'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<performance_analysis.RetrieverPerformanceAnalyzer at 0x7f6615c1f390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to analyze dataset: Loan Synthetic Data (s09)\n",
      "Performance analysis ready\n",
      "CPU times: user 199 ms, sys: 47 ms, total: 246 ms\n",
      "Wall time: 6.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run complete performance analysis\n",
    "# This will:\n",
    "# 1. Extract evaluation results from LangSmith\n",
    "# 2. Analyze performance, cost, and latency metrics\n",
    "# 3. Generate rankings and recommendations\n",
    "# 4. Create visualizations\n",
    "# 5. Save detailed reports\n",
    "\n",
    "# Use the dataset name from your LangSmith evaluation\n",
    "# dataset_name = \"Loan Synthetic Data (s09)\"\n",
    "\n",
    "# This approach:\n",
    "# 1. Gets dataset ID from dataset name\n",
    "# 2. Finds evaluation sessions that used the dataset\n",
    "# 3. Extracts all runs from those sessions\n",
    "# 4. Groups runs by retriever names\n",
    "# 5. Analyzes performance, cost, and latency\n",
    "\n",
    "# analyzer = analyze_retriever_performance(dataset_name=dataset_name, project_name_prefix=\"AIM - ADwLC - 2b1903b4\", days_back=7, save_report=True)\n",
    "# display(analyzer)\n",
    "# print(f\"Ready to analyze dataset: {dataset_name}\")\n",
    "\n",
    "# print(\"Performance analysis ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Step-by-Step Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "analyzer = RetrieverPerformanceAnalyzer()\n",
    "\n",
    "# Step 1: Extract experiment data\n",
    "# print(\"Step 1: Extracting experiment data from LangSmith...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to extract data:\n",
    "# results_df = analyzer.analyze_all_retrievers(days_back=7)\n",
    "# print(f\"Found data for {len(results_df)} experiments\")\n",
    "# results_df.head()\n",
    "\n",
    "# print(\"Data extraction ready - uncomment above to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create performance summary\n",
    "# Uncomment to run:\n",
    "# summary = analyzer.create_performance_summary()\n",
    "# for retriever, metrics in summary.items():\n",
    "#     print(f\"\\n{retriever}:\")\n",
    "#     print(f\"  - Avg Cost per Run: ${metrics.get('avg_cost_per_run', 0):.4f}\")\n",
    "#     print(f\"  - Avg Latency: {metrics.get('avg_latency', 0):.2f}s\")\n",
    "#     print(f\"  - QA Score: {metrics.get('qa_score', 'N/A')}\")\n",
    "\n",
    "# print(\"Performance summary ready - uncomment above to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate rankings\n",
    "# Uncomment to run:\n",
    "# rankings = analyzer.rank_retrievers()\n",
    "\n",
    "# print(\"🏆 RANKINGS 🏆\")\n",
    "# print(\"\\n📈 By Performance (Higher is Better):\")\n",
    "# for i, item in enumerate(rankings['by_performance'][:3], 1):\n",
    "#     print(f\"  {i}. {item['name']}: {item['performance']:.3f}\")\n",
    "\n",
    "# print(\"\\n💰 By Cost (Lower is Better):\")\n",
    "# for i, item in enumerate(rankings['by_cost'][:3], 1):\n",
    "#     print(f\"  {i}. {item['name']}: ${item['cost']:.4f}\")\n",
    "\n",
    "# print(\"\\n⚡ By Latency (Lower is Better):\")\n",
    "# for i, item in enumerate(rankings['by_latency'][:3], 1):\n",
    "#     print(f\"  {i}. {item['name']}: {item['latency']:.2f}s\")\n",
    "\n",
    "# print(\"\\n🎯 Overall Ranking (Weighted):\")\n",
    "# for i, item in enumerate(rankings['by_overall'][:3], 1):\n",
    "#     print(f\"  {i}. {item['name']}: {item['overall_score']:.3f}\")\n",
    "\n",
    "# print(\"Rankings ready - uncomment above to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate full report and save results\n",
    "# Uncomment to run:\n",
    "# report = analyzer.generate_analysis_report()\n",
    "# print(report)\n",
    "\n",
    "# # Save all results\n",
    "# analyzer.save_analysis(\"retriever_analysis_report.md\")\n",
    "\n",
    "# print(\"Report generation ready - uncomment above to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create visualizations\n",
    "# Uncomment to run:\n",
    "# analyzer.create_visualizations()\n",
    "\n",
    "# print(\"Visualization creation ready - uncomment above to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analysis Framework\n",
    "\n",
    "The performance analysis script provides comprehensive evaluation across three key dimensions:\n",
    "\n",
    "#### 📊 **Performance Metrics**\n",
    "- **QA Score**: Correctness of answers based on reference ground truth\n",
    "- **Helpfulness Score**: How helpful responses are to users\n",
    "- **Empathy Score**: Empathetic quality of responses\n",
    "\n",
    "#### 💰 **Cost Analysis**\n",
    "- **Total Cost**: Cumulative cost across all evaluation runs\n",
    "- **Cost per Run**: Average cost per individual evaluation\n",
    "- **Token Usage**: Input and output token consumption\n",
    "\n",
    "#### ⚡ **Latency Analysis**\n",
    "- **Average Latency**: Mean response time per retriever\n",
    "- **Total Processing Time**: Cumulative time across evaluations\n",
    "- **Latency Distribution**: Variation in response times\n",
    "\n",
    "#### 🎯 **Overall Ranking**\n",
    "Uses weighted scoring:\n",
    "- **40%** Performance (accuracy/quality)\n",
    "- **30%** Cost efficiency \n",
    "- **30%** Speed/latency\n",
    "\n",
    "This provides a balanced view for different use cases and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Direct Analysis from Evaluate Results\n",
    "\n",
    "The most accurate approach is to analyze the results returned directly from the `evaluate()` function calls. This gives us immediate access to all metrics without needing to query LangSmith again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Collect Results During Evaluation\n",
    "\n",
    "Instead of the current evaluation loop, modify it to collect the `evaluate()` results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imported evaluate results analyzer\n"
     ]
    }
   ],
   "source": [
    "# Import the new analyzer\n",
    "# from performance_analysis_from_evaluate import EvaluateResultsAnalyzer, analyze_from_evaluate_results\n",
    "from evaluation_cache2 import save_evaluation_results, load_evaluation_results\n",
    "\n",
    "print(\"✅ Imported evaluate results analyzer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Analyze Collected Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4898a1515794b27a6d4af4933789d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'naive_retrieval_chain-b210c0d3' at:\n",
      "https://smith.langchain.com/o/4a563880-75b7-483f-b9cd-cf740f81427b/datasets/8fab2835-2938-4c6a-9e6d-55e84c59a784/compare?selectedSessions=17ee44f7-5020-4528-ad21-c18d738d39d2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f82492094749e392c338989a33182c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever_chains_list = {\n",
    "    \"naive_retrieval_chain\" : naive_retrieval_chain,\n",
    "    \"bm25_retrieval_chain\": bm25_retrieval_chain,\n",
    "    \"contextual_compression_retrieval_chain\": contextual_compression_retrieval_chain,\n",
    "    \"multi_query_retrieval_chain\": multi_query_retrieval_chain,\n",
    "    \"parent_document_retrieval_chain\": parent_document_retrieval_chain,\n",
    "    \"ensemble_retrieval_chain\": ensemble_retrieval_chain,\n",
    "    \"semantic_retrieval_chain\": semantic_retrieval_chain\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"evaluation_results.pkl\"):\n",
    "    evaluation_results = {}\n",
    "    retriever_eval_progress_bar = tqdm(retriever_chains_list)\n",
    "    for retriever_chain in retriever_eval_progress_bar:\n",
    "        gc.collect()\n",
    "        chain_stage_filename = f\"{pipeline_stages_folder_name}/{retriever_chain}\"\n",
    "        if os.path.exists(chain_stage_filename):\n",
    "            print(f\"{retriever_chain} already processed, skipping to the next one...\")\n",
    "            continue\n",
    "    \n",
    "        retriever_eval_progress_bar.set_description(retriever_chain, refresh=True)\n",
    "        chain_to_invoke = retriever_chains_list[retriever_chain]\n",
    "        try:\n",
    "            result = evaluate(\n",
    "              chain_to_invoke.invoke,\n",
    "              data=dataset_name,\n",
    "              evaluators=[qa_evaluator, labeled_helpfulness_evaluator, empathy_evaluator],\n",
    "              num_repetitions=3,\n",
    "              max_concurrency=1,  # Limit concurrent requests to avoid rate limits\n",
    "              metadata={\"revision_id\": retriever_chain},\n",
    "              experiment_prefix=retriever_chain\n",
    "            )\n",
    "            \n",
    "            # Store the result for analysis\n",
    "            evaluation_results[retriever_chain] = result\n",
    "            write_to_file(retriever_chain, f\"revision_id: {retriever_chain}\")\n",
    "            print(f\"Finished evaluating and saving {retriever_chain} moving to the next one...\")\n",
    "        except Exception as ex:\n",
    "            print(f\"Failed to run evaluation on the {retriever_chain}, due to {ex}, skipping to the next one...\")\n",
    "            continue\n",
    "\n",
    "    save_evaluation_results(evaluation_results, \"evaluation_results.pkl\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_evaluation_results(evaluation_results, \"evaluation_results.pkl\")\n",
    "# evaluation_results[retriever_chain].to_pandas().columns\n",
    "# from pprint import pprint\n",
    "# pprint(evaluation_results[retriever_chain].__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from performance_analysis_revised2 import analyze_from_evaluate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded evaluation results from: evaluation_results.pkl\n",
      "📅 Cached on: 2025-07-27T00:59:20.798474\n",
      "📊 Found 1 retriever evaluations:\n",
      "   - naive_retrieval_chain\n",
      "CPU times: user 12.5 ms, sys: 2.38 ms, total: 14.9 ms\n",
      "Wall time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cached_results = load_evaluation_results(\"evaluation_results.pkl\")\n",
    "# evaluation_results\n",
    "# (cached_results['naive_retrieval_chain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded evaluation data from: retriever_performance_analysis_evaluation_data.pkl\n",
      "   - Save time: 2025-07-27T00:57:12.511757\n",
      "   - Retrievers: 1\n",
      "   - Total runs: 11\n",
      "   - Total cost: $0.000000\n",
      "✅ Loaded existing evaluation results from cache\n",
      "📊 Found 1 retriever strategies\n",
      "\n",
      "================================================================================\n",
      "QUICK PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 Naive\n",
      "   Runs: 11\n",
      "   Cost: $0.000000 ($0.000000/run)\n",
      "   Latency: 3.10s avg\n",
      "   Correctness: 0.636 avg, 63.6% success\n",
      "   Helpfulness: 0.091 avg, 9.1% success\n",
      "   Empathy: 0.000 avg, 0.0% success\n",
      "\n",
      "💾 Saving analysis results...\n",
      "📄 Report saved: retriever_performance_from_evaluate.md\n",
      "📊 Data saved: retriever_performance_from_evaluate_data.csv\n",
      "📋 Summary saved: retriever_performance_from_evaluate_summary.json\n",
      "🔍 Detailed results saved: retriever_performance_from_evaluate_evaluation_results.json\n",
      "💾 Evaluation data saved for reloading: retriever_performance_from_evaluate_evaluation_data.pkl\n",
      "📋 Evaluation data (JSON) saved: retriever_performance_from_evaluate_evaluation_data.json\n",
      "Analysis complete!\n",
      "Results DataFrame shape: (1, 27)\n",
      "Retrievers analyzed: ['naive_retrieval_chain']\n",
      "Analysis code ready - uncomment when evaluation_results is populated\n"
     ]
    }
   ],
   "source": [
    "# After collecting all evaluation results, analyze them\n",
    "# Uncomment when you have evaluation_results populated:\n",
    "\n",
    "# cached_results = load_evaluation_results(\"evaluation_results.pkl\")\n",
    "\n",
    "cached_results = evaluation_results\n",
    "\n",
    "# Run the analysis\n",
    "analyzer = analyze_from_evaluate_results(cached_results)\n",
    "\n",
    "# experiment_id = str(evaluate_result.experiment_id) if hasattr(evaluate_result, 'experiment_id') else (str(evaluate_result._experiment_id) if hasattr(evaluate_result, '_experiment_id') else 'unknown')\n",
    "\n",
    "# Save comprehensive results\n",
    "analyzer.save_analysis(\"retriever_performance_from_evaluate.md\")\n",
    "\n",
    "# Access specific data\n",
    "results_df = analyzer.results_df\n",
    "summary = analyzer.analysis_summary\n",
    "rankings = analyzer.rank_retrievers()\n",
    "\n",
    "print(\"Analysis complete!\")\n",
    "print(f\"Results DataFrame shape: {results_df.shape}\")\n",
    "print(f\"Retrievers analyzed: {list(summary.keys())}\")\n",
    "\n",
    "print(\"Analysis code ready - uncomment when evaluation_results is populated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
