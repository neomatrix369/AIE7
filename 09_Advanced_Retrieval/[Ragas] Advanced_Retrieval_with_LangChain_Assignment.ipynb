{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-IqJAMkwnCF"
   },
   "source": [
    "# [RAGAS] Advanced Retrieval with LangChain\n",
    "\n",
    "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
    "\n",
    "We'll touch on:\n",
    "\n",
    "- Naive Retrieval\n",
    "- Best-Matching 25 (BM25)\n",
    "- Multi-Query Retrieval\n",
    "- Parent-Document Retrieval\n",
    "- Contextual Compression (a.k.a. Rerank)\n",
    "- Ensemble Retrieval\n",
    "- Semantic chunking\n",
    "\n",
    "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
    "\n",
    "There will be two breakout rooms:\n",
    "\n",
    "- ðŸ¤ Breakout Room Part #1\n",
    "  - Task 1: Getting Dependencies!\n",
    "  - Task 2: Data Collection and Preparation\n",
    "  - Task 3: Setting Up QDrant!\n",
    "  - Task 4-10: Retrieval Strategies\n",
    "- ðŸ¤ Breakout Room Part #2\n",
    "  - Activity: Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rKP3hgHivpe"
   },
   "source": [
    "# ðŸ¤ Breakout Room Part #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xes8oT-xHN7"
   },
   "source": [
    "## Task 1: Getting Dependencies!\n",
    "\n",
    "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7OHJXzfyJyA"
   },
   "source": [
    "We'll also provide our OpenAI key, as well as our Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc;\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_env_var_is_set(env_var_name: str, human_readable_string: str = \"API Key\"):\n",
    "    api_key = os.getenv(env_var_name)\n",
    "  \n",
    "    if api_key:\n",
    "       print(f\"{env_var_name} is present\")\n",
    "    else:\n",
    "      print(f\"{env_var_name} is NOT present, paste key at the prompt:\")\n",
    "      os.environ[env_var_name] = getpass.getpass(f\"Please enter your {human_readable_string}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LttlDQUYgSI",
    "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is present\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "\n",
    "check_if_env_var_is_set(\"OPENAI_API_KEY\", \"OpenAI API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iUahNiJyQbv",
    "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHERE_API_KEY is present\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")\n",
    "\n",
    "check_if_env_var_is_set(\"COHERE_API_KEY\", \"Cohere API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw304iAFyRtl"
   },
   "source": [
    "## Task 2: Data Collection and Preparation\n",
    "\n",
    "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A92NC2QZzCsi"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GshBjVRJZ6p8"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./data/complaints.csv\",\n",
    "    metadata_columns=[\n",
    "      \"Date received\", \n",
    "      \"Product\", \n",
    "      \"Sub-product\", \n",
    "      \"Issue\", \n",
    "      \"Sub-issue\", \n",
    "      \"Consumer complaint narrative\", \n",
    "      \"Company public response\", \n",
    "      \"Company\", \n",
    "      \"State\", \n",
    "      \"ZIP code\", \n",
    "      \"Tags\", \n",
    "      \"Consumer consent provided?\", \n",
    "      \"Submitted via\", \n",
    "      \"Date sent to company\", \n",
    "      \"Company response to consumer\", \n",
    "      \"Timely response?\", \n",
    "      \"Consumer disputed?\", \n",
    "      \"Complaint ID\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "loan_complaint_data = loader.load()\n",
    "\n",
    "for doc in loan_complaint_data:\n",
    "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gQphb6y0C0S"
   },
   "source": [
    "Let's look at an example document to see if everything worked as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkUkCf7DaMiq",
    "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_complaint_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWaQpdHl0Gzc"
   },
   "source": [
    "## Task 3: Setting up QDrant!\n",
    "\n",
    "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
    "\n",
    "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
    "\n",
    "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NT8ihRJbYmMT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "small_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    loan_complaint_data,\n",
    "    small_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LoanComplaints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2SS4Rh0hiN"
   },
   "source": [
    "## Task 4: Naive RAG Chain\n",
    "\n",
    "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEH7X5Ai08FH"
   },
   "source": [
    "### R - Retrieval\n",
    "\n",
    "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
    "\n",
    "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GFDPrNBtb72o"
   },
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbBhyQjz06dx"
   },
   "source": [
    "### A - Augmented\n",
    "\n",
    "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7uSz-Dbqcoki"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlRzpb231GGJ"
   },
   "source": [
    "### G - Generation\n",
    "\n",
    "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c-1t9H60dJLg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\",                       \n",
    "    temperature=0.1,      # Lower temperature for more consistent outputs\n",
    "    request_timeout=120   # Longer timeout for complex operations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg3QRGzA1M2x"
   },
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "We're going to use LCEL to construct our chain.\n",
    "\n",
    "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0bvstS7mdOW3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 ms, sys: 3.48 ms, total: 23.9 ms\n",
      "Wall time: 27.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izKujhNb1ZG8"
   },
   "source": [
    "Let's see how this simple chain does on a few different prompts.\n",
    "\n",
    "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LI-5ueEddku9",
    "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 Î¼s, sys: 1e+03 ns, total: 6 Î¼s\n",
      "Wall time: 11.2 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "43zdcdUydtXh",
    "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 Î¼s, sys: 2 Î¼s, total: 8 Î¼s\n",
      "Wall time: 13.1 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "lpG6rlvvvKFq",
    "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Î¼s, sys: 1 Î¼s, total: 4 Î¼s\n",
      "Wall time: 6.68 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbfQmbr1leg"
   },
   "source": [
    "Overall, this is not bad! Let's see if we can make it better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft1vt8HPR16w"
   },
   "source": [
    "## Task 5: Best-Matching 25 (BM25) Retriever\n",
    "\n",
    "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
    "\n",
    "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
    "\n",
    "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qdF4wuj5R-cG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIjJlBQ8drKH"
   },
   "source": [
    "We'll construct the same chain - only changing the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WR15EQG7SLuw"
   },
   "outputs": [],
   "source": [
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gi-yXCDdvJk"
   },
   "source": [
    "Let's look at the responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oY9qzmm3SOrF",
    "outputId": "4d4f450f-5978-460f-f242-b32407868353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 Î¼s, sys: 0 ns, total: 6 Î¼s\n",
      "Wall time: 9.3 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvg5xHaUdxCl"
   },
   "source": [
    "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-dcbFn2vpZF"
   },
   "source": [
    "## Task 6: Contextual Compression (Using Reranking)\n",
    "\n",
    "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
    "\n",
    "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
    "\n",
    "The basic idea here is this:\n",
    "\n",
    "- We retrieve lots of documents that are very likely related to our query vector\n",
    "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
    "\n",
    "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
    "\n",
    "All we need to do is the following:\n",
    "\n",
    "- Create a basic retriever\n",
    "- Create a compressor (reranker, in this case)\n",
    "\n",
    "That's it!\n",
    "\n",
    "Let's see it in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "psHvO2K1v_ZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TA9RB2x-j7P"
   },
   "source": [
    "Let's create our chain again, and see how this does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1BXqmxvHwX6T"
   },
   "outputs": [],
   "source": [
    "contextual_compression_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V3iGpokswcBb",
    "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57 Î¼s, sys: 12 Î¼s, total: 69 Î¼s\n",
      "Wall time: 77 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEbT0g2S-mZ4"
   },
   "source": [
    "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqbghrBEQNn5"
   },
   "source": [
    "## Task 7: Multi-Query Retriever\n",
    "\n",
    "Typically in RAG we have a single query - the one provided by the user.\n",
    "\n",
    "What if we had....more than one query!\n",
    "\n",
    "In essence, a Multi-Query Retriever works by:\n",
    "\n",
    "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
    "2. Retrieving documents for each query.\n",
    "3. Using all unique retrieved documents as context\n",
    "\n",
    "So, how is it to set-up? Not bad! Let's see it down below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pfM26ReXQjzU"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1vRc129jQ5WW"
   },
   "outputs": [],
   "source": [
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "CGgNuOb3Q3M9",
    "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Î¼s, sys: 1 Î¼s, total: 5 Î¼s\n",
      "Wall time: 7.63 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDEawBf_d_3G"
   },
   "source": [
    "## Task 8: Parent Document Retriever\n",
    "\n",
    "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
    "\n",
    "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
    "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
    "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
    "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
    "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
    "\n",
    "Okay, maybe that was a few steps - but the basic idea is this:\n",
    "\n",
    "- Search for small documents\n",
    "- Return big documents\n",
    "\n",
    "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
    "\n",
    "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "qJ53JJuMd_ZH"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "parent_docs = loan_complaint_data\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpXfVUH3gL3"
   },
   "source": [
    "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
    "\n",
    "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzFc-_9HlGQ-",
    "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76/921369447.py:6: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
      "  parent_document_vectorstore = Qdrant(\n"
     ]
    }
   ],
   "source": [
    "vectorstore.client.create_collection(\n",
    "  collection_name=\"full_documents\",\n",
    "  vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = Qdrant(\n",
    "  client=vectorstore.client,     # âœ… Reuse existing client\n",
    "  embeddings=small_embeddings,         # âœ… Reuse embeddings\n",
    "  collection_name=\"full_documents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_g95FA3s6w"
   },
   "source": [
    "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BpWVjPf4fLUp"
   },
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoYmSWfE32Zo"
   },
   "source": [
    "By default, this is empty as we haven't added any documents - let's add some now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iQ2ZzfKigMZc"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI7Tip1335rE"
   },
   "source": [
    "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Qq_adt2KlSqp"
   },
   "outputs": [],
   "source": [
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNolUVQb4Apt"
   },
   "source": [
    "Let's give it a whirl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "TXB5i89Zly5W",
    "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Î¼s, sys: 1 Î¼s, total: 4 Î¼s\n",
      "Wall time: 6.68 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B41cj42s4DPM"
   },
   "source": [
    "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUrIBKl_TwS9"
   },
   "source": [
    "## Task 9: Ensemble Retriever\n",
    "\n",
    "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
    "\n",
    "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
    "\n",
    "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "8j7jpZsKTxic"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpo9Psl5hhJ-"
   },
   "source": [
    "We'll pack *all* of these retrievers together in an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "KZ__EZwpUKkd"
   },
   "outputs": [],
   "source": [
    "ensemble_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSsvHpRMj24L"
   },
   "source": [
    "Let's look at our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lMvqL88UQI-",
    "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Î¼s, sys: 1e+03 ns, total: 5 Î¼s\n",
      "Wall time: 8.34 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MopbkNJAXVaN"
   },
   "source": [
    "## Task 10: Semantic Chunking\n",
    "\n",
    "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
    "\n",
    "Essentially, Semantic Chunking is implemented by:\n",
    "\n",
    "1. Embedding all sentences in the corpus.\n",
    "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
    "  - `percentile`\n",
    "  - `standard_deviation`\n",
    "  - `interquartile`\n",
    "  - `gradient`\n",
    "3. Each sequence of related sentences is kept as a document!\n",
    "\n",
    "Let's see how to implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `breakpoint_threshold_type` parameter controls when the semantic chunker creates chunk boundaries based on embedding similarity between sentences:\n",
    "\n",
    "**Four Threshold Types:**\n",
    "\n",
    "1. _\"percentile\" (default)_\n",
    "- Splits when sentence embedding distance exceeds the 95th percentile of all distances\n",
    "- Effect: Creates chunks at the most semantically distinct boundaries\n",
    "- Behavior: More conservative splitting, larger chunks\n",
    "\n",
    "2. _\"standard_deviation\"_\n",
    "- Splits when distance exceeds 3 standard deviations from mean\n",
    "- Effect: Better predictable performance, especially for normally distributed content\n",
    "- Behavior: More consistent chunk sizes\n",
    "\n",
    "3. _\"interquartile\"_\n",
    "- Uses IQR * 1.5 scaling factor to determine breakpoints\n",
    "- Effect: Middle-ground approach, robust to outliers\n",
    "- Behavior: Balanced chunk distribution\n",
    "\n",
    "4. _\"gradient\"_\n",
    "- Detects anomalies in embedding distance gradients\n",
    "- Effect: Best for domain-specific/highly correlated content\n",
    "- Behavior: Finds subtle semantic transitions\n",
    "\n",
    "**Impact:** _The threshold type determines sensitivity to semantic changes - more sensitive types create smaller, more focused chunks while less sensitive types create larger, more comprehensive chunks._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ciZbFEldv_"
   },
   "source": [
    "We'll use the `percentile` thresholding method for this example which will:\n",
    "\n",
    "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "66EIEWiEYl5y"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    small_embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqoKmz12mhRW"
   },
   "source": [
    "Now we can split our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ROcV7o68ZIq7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 271 ms, sys: 30.8 ms, total: 302 ms\n",
      "Wall time: 9.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-LNC-Xmjex"
   },
   "source": [
    "Let's create a new vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "h3sl9QjyZhIe"
   },
   "outputs": [],
   "source": [
    "vectorstore.client.create_collection(\n",
    "  collection_name=\"Loan_Complaint_Data_Semantic_Chunks\",\n",
    "  vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "semantic_vectorstore = Qdrant(\n",
    "  client=vectorstore.client,     # âœ… Reuse existing client\n",
    "  embeddings=small_embeddings,         # âœ… Reuse embeddings\n",
    "  collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
    ")\n",
    "\n",
    "# Add documents after creation\n",
    "_ = semantic_vectorstore.add_documents(semantic_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh_r_-LHmmKn"
   },
   "source": [
    "We'll use naive retrieval for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "odVyDUHwZftc"
   },
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkeiv_ojmp6G"
   },
   "source": [
    "Finally we can create our classic chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "xWE_0J0mZveG"
   },
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5pfjLQ3ms9_"
   },
   "source": [
    "And view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lN2j-e4Z0SD",
    "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk2n3-pnVWDJ"
   },
   "source": [
    "# ðŸ¤ Breakout Room Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SkJLYwMVZkj"
   },
   "source": [
    "#### ðŸ—ï¸ Activity #1\n",
    "\n",
    "Your task is to evaluate the various Retriever methods against eachother.\n",
    "\n",
    "You are expected to:\n",
    "\n",
    "1. Create a \"golden dataset\"\n",
    " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
    "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
    " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
    "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
    "\n",
    "Your analysis should factor in:\n",
    "  - Cost\n",
    "  - Latency\n",
    "  - Performance\n",
    "\n",
    "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWAr16a5XMub"
   },
   "source": [
    "##### HINTS:\n",
    "\n",
    "- LangSmith provides detailed information about latency and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "tgDICngKXLGK"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_env_var_is_set(env_var_name: str, human_readable_string: str = \"API Key\"):\n",
    "    api_key = os.getenv(env_var_name)\n",
    "  \n",
    "    if api_key:\n",
    "       print(f\"{env_var_name} is present\")\n",
    "    else:\n",
    "      print(f\"{env_var_name} is NOT present, paste key at the prompt:\")\n",
    "      os.environ[env_var_name] = getpass.getpass(f\"Please enter your {human_readable_string}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original documents count: 825\n",
      "Documents count after filtering: 480\n"
     ]
    }
   ],
   "source": [
    "# docs = loan_complaint_data.copy()\n",
    "print(f\"Original documents count: {len(loan_complaint_data)}\")\n",
    "\n",
    "filtered_docs = []\n",
    "for doc in loan_complaint_data:\n",
    "    narrative = doc.metadata.get(\"Consumer complaint narrative\", \"\")\n",
    "    if (len(narrative.strip()) < 100 or \n",
    "        narrative.count(\"XXXX\") > 5 or \n",
    "        narrative.strip() in [\"\", \"None\", \"N/A\"]):\n",
    "        continue\n",
    "\n",
    "    doc.page_content = f\"Customer Issue: {doc.metadata.get('Issue', 'Unknown')}\\n\"\n",
    "    doc.page_content += f\"Product: {doc.metadata.get('Product', 'Unknown')}\\n\"\n",
    "    doc.page_content += f\"Complaint Details: {narrative}\"\n",
    "\n",
    "    filtered_docs.append(doc)\n",
    "\n",
    "print(f\"Documents count after filtering: {len(filtered_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",  # Less capable than mini for reasoning tasks, but okay for the task\n",
    "    temperature=0.1,      # Lower temperature for more consistent outputs\n",
    "    request_timeout=120   # Longer timeout for complex operations\n",
    "))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatOpenAI(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        temperature=0.1,      # Lower temperature for more consistent outputs\n",
    "        request_timeout=120   # Longer timeout for complex operations        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "transformer_llm = generator_llm\n",
    "embedding_model = generator_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before generation: 484.4 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Check memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Memory usage before generation: {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(\n",
    "                llm=generator_llm, embedding_model=embedding_model, \n",
    "                #knowledge_graph=loan_data_kg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80209e57df74608b91848b271496a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b45ad245ea1428089ff6ebbe0d3a395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Node bc149e2a-4864-4d8e-915c-ae5c61dbb8bb does not have a summary. Skipping filtering.\n",
      "Node 27daea41-fe22-4363-b130-2eb07abc95f4 does not have a summary. Skipping filtering.\n",
      "Node 79cb6a7a-bf39-4a56-b2ae-d26f88f5f2e1 does not have a summary. Skipping filtering.\n",
      "Node c3b82972-0700-4233-becb-6239f293b25c does not have a summary. Skipping filtering.\n",
      "Node 36b7a2ea-7cc9-4f32-8e57-44d346f6e869 does not have a summary. Skipping filtering.\n",
      "Node 3fdc6de8-ff46-4e3c-9e1a-05b5f8397ded does not have a summary. Skipping filtering.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7aedf5131c047ab958c9ede65c6e404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060219c3a22e4f33b9099e0b13dd6084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa3a69f2e81485ea5411eff00739ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c82a5f9d2d47648f374534c3cfe2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af668ba2af2046588464cffb3ca88043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 s, sys: 234 ms, total: 1.9 s\n",
      "Wall time: 2min 27s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you please provide me with the detailed Co...</td>\n",
       "      <td>[Customer Issue: Dealing with your lender or s...</td>\n",
       "      <td>Customer Issue: Dealing with your lender or se...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What issues are being reported with Aidvantage...</td>\n",
       "      <td>[Customer Issue: Dealing with your lender or s...</td>\n",
       "      <td>The customer reports that Aidvantage assigned ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does FERPA protect student loan borrowers ...</td>\n",
       "      <td>[Customer Issue: Dealing with your lender or s...</td>\n",
       "      <td>Customer Issue: Dealing with your lender or se...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What issues are being raised regarding Nelnet ...</td>\n",
       "      <td>[Customer Issue: Dealing with your lender or s...</td>\n",
       "      <td>The consumer is confused about the issuer of t...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does it mean that I was told I am in forb...</td>\n",
       "      <td>[Customer Issue: Dealing with your lender or s...</td>\n",
       "      <td>The context states that the borrower was told ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does FDCPA relate to the illegal student l...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCustomer Issue: Improper use of yo...</td>\n",
       "      <td>The FDCPA (Fair Debt Collection Practices Act)...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Based on the issues reported with Aidvantage, ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCustomer Issue: Dealing with your ...</td>\n",
       "      <td>The cases highlight that Aidvantage has failed...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does the Department of Education's abolish...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCustomer Issue: Improper use of yo...</td>\n",
       "      <td>The context indicates that following the Depar...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the violation of the FCRA by XXXX and...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nI am writing to formally dispute i...</td>\n",
       "      <td>The violation of the FCRA by XXXX and XXXX, wh...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the violation of the Family Education...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nCustomer Issue: Improper use of yo...</td>\n",
       "      <td>The violation of FERPA is directly related to ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Can you please provide me with the detailed Co...   \n",
       "1  What issues are being reported with Aidvantage...   \n",
       "2  How does FERPA protect student loan borrowers ...   \n",
       "3  What issues are being raised regarding Nelnet ...   \n",
       "4  What does it mean that I was told I am in forb...   \n",
       "5  How does FDCPA relate to the illegal student l...   \n",
       "6  Based on the issues reported with Aidvantage, ...   \n",
       "7  How does the Department of Education's abolish...   \n",
       "8  How does the violation of the FCRA by XXXX and...   \n",
       "9  How does the violation of the Family Education...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Customer Issue: Dealing with your lender or s...   \n",
       "1  [Customer Issue: Dealing with your lender or s...   \n",
       "2  [Customer Issue: Dealing with your lender or s...   \n",
       "3  [Customer Issue: Dealing with your lender or s...   \n",
       "4  [Customer Issue: Dealing with your lender or s...   \n",
       "5  [<1-hop>\\n\\nCustomer Issue: Improper use of yo...   \n",
       "6  [<1-hop>\\n\\nCustomer Issue: Dealing with your ...   \n",
       "7  [<1-hop>\\n\\nCustomer Issue: Improper use of yo...   \n",
       "8  [<1-hop>\\n\\nI am writing to formally dispute i...   \n",
       "9  [<1-hop>\\n\\nCustomer Issue: Improper use of yo...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Customer Issue: Dealing with your lender or se...   \n",
       "1  The customer reports that Aidvantage assigned ...   \n",
       "2  Customer Issue: Dealing with your lender or se...   \n",
       "3  The consumer is confused about the issuer of t...   \n",
       "4  The context states that the borrower was told ...   \n",
       "5  The FDCPA (Fair Debt Collection Practices Act)...   \n",
       "6  The cases highlight that Aidvantage has failed...   \n",
       "7  The context indicates that following the Depar...   \n",
       "8  The violation of the FCRA by XXXX and XXXX, wh...   \n",
       "9  The violation of FERPA is directly related to ...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_specific_query_synthesizer  \n",
       "7  multi_hop_specific_query_synthesizer  \n",
       "8  multi_hop_specific_query_synthesizer  \n",
       "9  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "testset = None\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "\n",
    "# if os.path.exists('golden-master.csv'):\n",
    "#     golden_dataset_df = pd.read_csv('golden-master.csv')\n",
    "#     golden_dataset_df['reference_contexts'] = golden_dataset_df['reference_contexts'].apply(eval)\n",
    "#     testset = EvaluationDataset.from_pandas(golden_dataset_df)\n",
    "# else:\n",
    "#     testset = generator.generate_with_langchain_docs(loan_complaint_data[:20], testset_size=10)\n",
    "testset = generator.generate_with_langchain_docs(loan_complaint_data[:20], testset_size=10)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after generation: 490.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Check memory usage\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Memory usage after generation: {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2935"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after gc.collect(): 490.8 MB\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Memory usage after gc.collect(): {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing dataset: Loan Synthetic Data (s09)\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "langsmith_client = Client(\n",
    "    timeout_ms=60000,  # 60 seconds\n",
    "    retry_config={\"max_retries\": 5}\n",
    ")\n",
    "\n",
    "dataset_name = \"Loan Synthetic Data (s09)\"\n",
    "\n",
    "existing_datasets = langsmith_client.list_datasets()\n",
    "dataset_exists = any(dataset.name == dataset_name for dataset in existing_datasets)\n",
    "\n",
    "if dataset_exists:\n",
    "  langsmith_dataset = langsmith_client.read_dataset(dataset_name=dataset_name)\n",
    "  print(f\"Using existing dataset: {dataset_name}\")\n",
    "else:\n",
    "  langsmith_dataset = langsmith_client.create_dataset(\n",
    "      dataset_name=dataset_name,\n",
    "      description=\"Loan Synthetic Data (for s09 exercise)\"\n",
    "  )\n",
    "  print(f\"Created new dataset: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy, \n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chains_list = {\n",
    "    \"naive_retrieval_chain\" : { 'rag_chain': naive_retrieval_chain },\n",
    "    \"bm25_retrieval_chain\": { 'rag_chain': bm25_retrieval_chain },\n",
    "    \"contextual_compression_retrieval_chain\": { 'rag_chain': contextual_compression_retrieval_chain },\n",
    "    \"multi_query_retrieval_chain\": { 'rag_chain': multi_query_retrieval_chain },\n",
    "    \"parent_document_retrieval_chain\": { 'rag_chain': parent_document_retrieval_chain },\n",
    "    \"ensemble_retrieval_chain\": { 'rag_chain': ensemble_retrieval_chain }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def simplest_copy_method(original_dataset):\n",
    "    \"\"\"\n",
    "    Simplest method: Use copy.deepcopy()\n",
    "    This creates a completely independent copy\n",
    "    \"\"\"\n",
    "    dataset_copy = copy.deepcopy(original_dataset)\n",
    "    return dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e51f94b35c45f6a5e7f6a362ebecae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "from ragas import EvaluationDataset\n",
    "from tqdm.notebook import tqdm\n",
    "for retriever_chain in tqdm(retriever_chains_list.keys()):\n",
    "    copy_of_testset = simplest_copy_method(testset)\n",
    "    retriever_chains_list[retriever_chain]['dataset'] = copy_of_testset\n",
    "    rag_chain = retriever_chains_list[retriever_chain]['rag_chain']\n",
    "    for test_row in copy_of_testset:\n",
    "        response = rag_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "        test_row.eval_sample.response = response[\"response\"].content\n",
    "        # test_row.eval_sample.metrics = response[\"response\"].usage\n",
    "        test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba36bf6ebaa4636bb262e4a0d44e401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for retriever_chain in tqdm(retriever_chains_list.keys()):\n",
    "    copy_of_testset = retriever_chains_list[retriever_chain]['dataset']\n",
    "    retriever_chains_list[retriever_chain]['evaluation_dataset'] = EvaluationDataset.from_pandas(copy_of_testset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_cache import save_evaluation_result, load_evaluation_result\n",
    "\n",
    "pipeline_stages_folder_name = \".pipeline-stages\"\n",
    "os.makedirs(pipeline_stages_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AgentGoalAccuracyWithReference', 'AgentGoalAccuracyWithoutReference', 'AnswerCorrectness', 'AnswerRelevancy', 'AnswerSimilarity', 'AspectCritic', 'BleuScore', 'ContextEntityRecall', 'ContextPrecision', 'ContextRecall', 'ContextUtilization', 'DataCompyScore', 'DistanceMeasure', 'ExactMatch', 'FactualCorrectness', 'Faithfulness', 'FaithfulnesswithHHEM', 'InstanceRubrics', 'LLMContextPrecisionWithReference', 'LLMContextPrecisionWithoutReference', 'LLMContextRecall', 'LLMSQLEquivalence', 'Metric', 'MetricOutputType', 'MetricType', 'MetricWithEmbeddings', 'MetricWithLLM', 'MultiModalFaithfulness', 'MultiModalRelevance', 'MultiTurnMetric', 'NoiseSensitivity', 'NonLLMContextPrecisionWithReference', 'NonLLMContextRecall', 'NonLLMStringSimilarity', 'ResponseRelevancy', 'RougeScore', 'RubricsScore', 'SemanticSimilarity', 'SimpleCriteriaScore', 'SingleTurnMetric', 'StringPresence', 'SummarizationScore', 'ToolCallAccuracy', 'TopicAdherenceScore', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_answer_correctness', '_answer_relevance', '_answer_similarity', '_aspect_critic', '_bleu_score', '_context_entities_recall', '_context_precision', '_context_recall', '_datacompy_score', '_domain_specific_rubrics', '_factual_correctness', '_faithfulness', '_goal_accuracy', '_instance_specific_rubrics', '_multi_modal_faithfulness', '_multi_modal_relevance', '_noise_sensitivity', '_rouge_score', '_simple_criteria', '_sql_semantic_equivalence', '_string', '_summarization', '_tool_call_accuracy', '_topic_adherence', 'answer_correctness', 'answer_relevancy', 'answer_similarity', 'base', 'context_entity_recall', 'context_precision', 'context_recall', 'faithfulness', 'multimodal_faithness', 'multimodal_relevance', 'summarization_score', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import ragas.metrics\n",
    "print(dir(ragas.metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6416d619754ee5965094b1aa78760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff4fef8350440098ac5d94b6ed4f43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[21]: AttributeError('StringIO' object has no attribute 'statements')\n",
      "Exception raised in Job[45]: AttributeError('StringIO' object has no attribute 'statements')\n",
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[37]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[61]: TimeoutError()\n",
      "Exception raised in Job[69]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving naive_retrieval_chain...\n",
      "ðŸ”„ Extracting serializable data from evaluation results...\n",
      "   Processing retriever...\n",
      "âœ… Evaluation results cached to: .pipeline-stages/ragas_evaluation_results_naive_retrieval_chain.pkl\n",
      "ðŸ“‹ JSON version saved to: .pipeline-stages/ragas_evaluation_results_naive_retrieval_chain.json\n",
      "ðŸ“‹ Metadata saved to: .pipeline-stages/ragas_evaluation_results_naive_retrieval_chain_metadata.json\n",
      "Finished evaluating and saving naive_retrieval_chain moving to the next one...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aa6800ac4341b998821bfbb6ba9f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving bm25_retrieval_chain...\n",
      "ðŸ”„ Extracting serializable data from evaluation results...\n",
      "   Processing retriever...\n",
      "âœ… Evaluation results cached to: .pipeline-stages/ragas_evaluation_results_bm25_retrieval_chain.pkl\n",
      "ðŸ“‹ JSON version saved to: .pipeline-stages/ragas_evaluation_results_bm25_retrieval_chain.json\n",
      "ðŸ“‹ Metadata saved to: .pipeline-stages/ragas_evaluation_results_bm25_retrieval_chain_metadata.json\n",
      "Finished evaluating and saving bm25_retrieval_chain moving to the next one...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84912a547ef646b0913e3d3e9d8f0140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[21]: AttributeError('StringIO' object has no attribute 'statements')\n",
      "Exception raised in Job[45]: AttributeError('StringIO' object has no attribute 'statements')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving contextual_compression_retrieval_chain...\n",
      "ðŸ”„ Extracting serializable data from evaluation results...\n",
      "   Processing retriever...\n",
      "âœ… Evaluation results cached to: .pipeline-stages/ragas_evaluation_results_contextual_compression_retrieval_chain.pkl\n",
      "ðŸ“‹ JSON version saved to: .pipeline-stages/ragas_evaluation_results_contextual_compression_retrieval_chain.json\n",
      "ðŸ“‹ Metadata saved to: .pipeline-stages/ragas_evaluation_results_contextual_compression_retrieval_chain_metadata.json\n",
      "Finished evaluating and saving contextual_compression_retrieval_chain moving to the next one...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fa7a387e22414ba4cfdb9f5ed10fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[45]: AttributeError('StringIO' object has no attribute 'statements')\n",
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[13]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[37]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[61]: TimeoutError()\n",
      "Exception raised in Job[69]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving multi_query_retrieval_chain...\n",
      "ðŸ”„ Extracting serializable data from evaluation results...\n",
      "   Processing retriever...\n",
      "âœ… Evaluation results cached to: .pipeline-stages/ragas_evaluation_results_multi_query_retrieval_chain.pkl\n",
      "ðŸ“‹ JSON version saved to: .pipeline-stages/ragas_evaluation_results_multi_query_retrieval_chain.json\n",
      "ðŸ“‹ Metadata saved to: .pipeline-stages/ragas_evaluation_results_multi_query_retrieval_chain_metadata.json\n",
      "Finished evaluating and saving multi_query_retrieval_chain moving to the next one...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05608cc0e6554d32ac7aa882c02c4827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving parent_document_retrieval_chain...\n",
      "ðŸ”„ Extracting serializable data from evaluation results...\n",
      "   Processing retriever...\n",
      "âœ… Evaluation results cached to: .pipeline-stages/ragas_evaluation_results_parent_document_retrieval_chain.pkl\n",
      "ðŸ“‹ JSON version saved to: .pipeline-stages/ragas_evaluation_results_parent_document_retrieval_chain.json\n",
      "ðŸ“‹ Metadata saved to: .pipeline-stages/ragas_evaluation_results_parent_document_retrieval_chain_metadata.json\n",
      "Finished evaluating and saving parent_document_retrieval_chain moving to the next one...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45d633102fc478387d2bc2c72e8a99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[45]: AttributeError('StringIO' object has no attribute 'statements')\n",
      "Exception raised in Job[21]: AttributeError('StringIO' object has no attribute 'statements')\n",
      "Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[13]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[36]: TimeoutError()\n",
      "Exception raised in Job[37]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[61]: TimeoutError()\n",
      "Exception raised in Job[69]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ensemble_retrieval_chain...\n",
      "ðŸ”„ Extracting serializable data from evaluation results...\n",
      "   Processing retriever...\n",
      "âœ… Evaluation results cached to: .pipeline-stages/ragas_evaluation_results_ensemble_retrieval_chain.pkl\n",
      "ðŸ“‹ JSON version saved to: .pipeline-stages/ragas_evaluation_results_ensemble_retrieval_chain.json\n",
      "ðŸ“‹ Metadata saved to: .pipeline-stages/ragas_evaluation_results_ensemble_retrieval_chain_metadata.json\n",
      "Finished evaluating and saving ensemble_retrieval_chain moving to the next one...\n",
      "CPU times: user 54.6 s, sys: 6.24 s, total: 1min\n",
      "Wall time: 39min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from ragas.metrics import LLMContextRecall, LLMContextPrecisionWithoutReference, LLMContextPrecisionWithReference, NonLLMContextPrecisionWithReference \n",
    "from ragas.metrics import ContextEntityRecall, NoiseSensitivity, Faithfulness, MultiModalFaithfulness\n",
    "# from ragas.metrics import ContextRelevance -- not available for current version of RAGAS\n",
    "from ragas import evaluate, RunConfig\n",
    "from ragas.cost import get_token_usage_for_openai\n",
    "\n",
    "evaluation_results = {}\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "for retriever_chain in tqdm(retriever_chains_list.keys()):\n",
    "    evaluation_results_filename = f\"{pipeline_stages_folder_name}/ragas_evaluation_results_{retriever_chain}.pkl\"\n",
    "    if os.path.exists(evaluation_results_filename):\n",
    "        print(f\"{retriever_chain} already processed, skipping to the next one...\")\n",
    "        retriever_chains_list[retriever_chain]['evaluation_result'] = load_evaluation_result(evaluation_results_filename)\n",
    "        continue\n",
    "\n",
    "    result = evaluate(\n",
    "        dataset=retriever_chains_list[retriever_chain]['evaluation_dataset'],\n",
    "        metrics=[\n",
    "            # STRONGLY related to retrievers  \n",
    "            LLMContextRecall(),          # Retrieval completeness\n",
    "            LLMContextPrecisionWithoutReference(), # Retrieval relevance  \n",
    "            LLMContextPrecisionWithReference(),\n",
    "            NonLLMContextPrecisionWithReference(),\n",
    "            ContextEntityRecall(),       # Entity-based retrieval quality\n",
    "            NoiseSensitivity(),          # Noise handling in retrieval\n",
    "            # ContextRelevance(),          # Overall context relevance to query -- not available for current version of RAGAS\n",
    "            # MILDLY related to retrievers \n",
    "            Faithfulness(),              # Keep - generation quality depends on retrieval\n",
    "            MultiModalFaithfulness()     # Add if using multimodal - context consistency\n",
    "        ],\n",
    "        llm=evaluator_llm,\n",
    "        token_usage_parser=get_token_usage_for_openai,\n",
    "        run_config=custom_run_config\n",
    "    )\n",
    "    print(f\"Saving {retriever_chain}...\")\n",
    "    retriever_chains_list[retriever_chain]['evaluation_result'] = result\n",
    "    save_evaluation_result(result, evaluation_results_filename)\n",
    "        \n",
    "    print(f\"Finished evaluating and saving {retriever_chain} moving to the next one...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Performance Analysis\n",
    "\n",
    "Now that we have evaluation data from LangSmith, let's analyze the performance of different retrievers across multiple dimensions: **Performance**, **Cost**, and **Latency**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ragas_metrics(ragas_result, model_name: str = ''):\n",
    "    \"\"\"Extract cost, latency, and token metrics from RAGAS evaluation result\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    def get_value(obj, key):\n",
    "        \"\"\"Get value from dict key or object attribute\"\"\"\n",
    "        return obj.get(key) if isinstance(obj, dict) else getattr(obj, key, None)\n",
    "    \n",
    "    def safe_mean(values):\n",
    "        \"\"\"Calculate mean, filtering out NaN values\"\"\"\n",
    "        if not values:\n",
    "            return 0\n",
    "        arr = np.array(values, dtype=float)\n",
    "        valid = arr[~np.isnan(arr)]\n",
    "        return float(np.mean(valid)) if len(valid) > 0 else 0\n",
    "\n",
    "    def get_model_costs(model_name):\n",
    "        PER_MILLION = 1_000_000\n",
    "        \"\"\"Get per-token costs for common models\"\"\"\n",
    "        costs = {\n",
    "            'gpt-4.1': (2.50 / PER_MILLION, 10.00 / PER_MILLION),\n",
    "            'gpt-4.1-nano': (0.15 / PER_MILLION, 0.60 / PER_MILLION),\n",
    "            'gpt-4.1-mini': (0.15 / PER_MILLION, 0.60 / PER_MILLION), \n",
    "            'gpt-4o-mini': (0.000000150, 0.000000600),\n",
    "            'gpt-4o': (0.000002500, 0.000010000),\n",
    "            'gpt-4-turbo': (0.000010000, 0.000030000),\n",
    "            'gpt-3.5-turbo': (0.000000500, 0.000001500),\n",
    "            'claude-3-haiku': (0.000000250, 0.000001250),\n",
    "            'claude-3-sonnet': (0.000003000, 0.000015000),\n",
    "            'claude-3-opus': (0.000015000, 0.000075000),\n",
    "            'text-embedding-3-small': (0.02 / PER_MILLION, 0.0),\n",
    "            'text-embedding-3-large': (0.13 / PER_MILLION, 0.0),\n",
    "            'rerank-v3.5': (2.00 / PER_MILLION, 0.0)\n",
    "        }\n",
    "        \n",
    "        # Try exact match, then partial match\n",
    "        if model_name in costs:\n",
    "            return costs[model_name]\n",
    "        \n",
    "        for model_key in costs:\n",
    "            if model_key in model_name.lower():\n",
    "                return costs[model_key]\n",
    "        \n",
    "        return costs['gpt-4o-mini']  # Default\n",
    "    \n",
    "    # Extract data\n",
    "    scores = get_value(ragas_result, 'scores') or []\n",
    "    scores_dict = get_value(ragas_result, '_scores_dict') or {}\n",
    "    cost_cb = get_value(ragas_result, 'cost_cb') or {}\n",
    "    usage_data = get_value(cost_cb, 'usage_data') or []\n",
    "    \n",
    "    # Calculate runs\n",
    "    total_runs = len(scores) if scores else 1\n",
    "    \n",
    "    # Calculate RAGAS scores (averages from score lists)\n",
    "    ragas_scores = {}\n",
    "    for metric, values in scores_dict.items():\n",
    "        if isinstance(values, list):\n",
    "            ragas_scores[metric] = safe_mean(values)\n",
    "    \n",
    "    # Calculate tokens and cost\n",
    "    total_input = sum(get_value(usage, 'input_tokens') or 0 for usage in usage_data)\n",
    "    total_output = sum(get_value(usage, 'output_tokens') or 0 for usage in usage_data)\n",
    "    \n",
    "    input_cost, output_cost = get_model_costs(model_name)\n",
    "    total_cost = (total_input * input_cost) + (total_output * output_cost)\n",
    "    \n",
    "    # Build metrics\n",
    "    metrics = {\n",
    "        'Total_Runs': total_runs,\n",
    "        'Total_Cost': total_cost,\n",
    "        'Total_Input_Tokens': total_input,\n",
    "        'Total_Output_Tokens': total_output,\n",
    "        'Total_Latency_Sec': 0,  # Not available in this data\n",
    "        'Avg_Cost_Per_Run': total_cost / total_runs,\n",
    "        'Avg_Input_Tokens_Per_Run': total_input / total_runs,\n",
    "        'Avg_Output_Tokens_Per_Run': total_output / total_runs,\n",
    "        'Avg_Latency_Sec': 0,\n",
    "        **ragas_scores\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fb3a146be54f578fe7b6ac668703df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_stats_df = pd.DataFrame()\n",
    "for retriever_chain in tqdm(retriever_chains_list.keys()):\n",
    "    result = retriever_chains_list[retriever_chain]['evaluation_result']\n",
    "    retriever_chains_list[retriever_chain]['evaluation_metrics'] = extract_ragas_metrics(result, 'gpt-4.1-mini').copy()\n",
    "    each_retriever_df = pd.concat([\n",
    "        pd.DataFrame([{\"retriever\": retriever_chain}]), \n",
    "        pd.DataFrame([retriever_chains_list[retriever_chain]['evaluation_metrics']])\n",
    "    ], axis=1)\n",
    "    raw_stats_df = pd.concat([\n",
    "        raw_stats_df, each_retriever_df\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_stats_df.to_csv('ragas_retriever_raw_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retriever</th>\n",
       "      <th>Total_Runs</th>\n",
       "      <th>Total_Cost</th>\n",
       "      <th>Total_Input_Tokens</th>\n",
       "      <th>Total_Output_Tokens</th>\n",
       "      <th>Total_Latency_Sec</th>\n",
       "      <th>Avg_Cost_Per_Run</th>\n",
       "      <th>Avg_Input_Tokens_Per_Run</th>\n",
       "      <th>Avg_Output_Tokens_Per_Run</th>\n",
       "      <th>Avg_Latency_Sec</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>llm_context_precision_without_reference</th>\n",
       "      <th>llm_context_precision_with_reference</th>\n",
       "      <th>non_llm_context_precision_with_reference</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>noise_sensitivity_relevant</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>faithful_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive_retrieval_chain</td>\n",
       "      <td>10</td>\n",
       "      <td>0.243176</td>\n",
       "      <td>659319</td>\n",
       "      <td>240463</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024318</td>\n",
       "      <td>65931.9</td>\n",
       "      <td>24046.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.794524</td>\n",
       "      <td>0.918197</td>\n",
       "      <td>0.777317</td>\n",
       "      <td>0.392897</td>\n",
       "      <td>0.320769</td>\n",
       "      <td>0.223647</td>\n",
       "      <td>0.808040</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bm25_retrieval_chain</td>\n",
       "      <td>10</td>\n",
       "      <td>0.150856</td>\n",
       "      <td>392780</td>\n",
       "      <td>153232</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015086</td>\n",
       "      <td>39278.0</td>\n",
       "      <td>15323.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.913889</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.413889</td>\n",
       "      <td>0.434487</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.870687</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contextual_compression_retrieval_chain</td>\n",
       "      <td>10</td>\n",
       "      <td>0.111962</td>\n",
       "      <td>280371</td>\n",
       "      <td>116510</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011196</td>\n",
       "      <td>28037.1</td>\n",
       "      <td>11651.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.639524</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.449487</td>\n",
       "      <td>0.406235</td>\n",
       "      <td>0.782676</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_query_retrieval_chain</td>\n",
       "      <td>10</td>\n",
       "      <td>0.254773</td>\n",
       "      <td>805097</td>\n",
       "      <td>223347</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025477</td>\n",
       "      <td>80509.7</td>\n",
       "      <td>22334.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844524</td>\n",
       "      <td>0.908392</td>\n",
       "      <td>0.850487</td>\n",
       "      <td>0.359511</td>\n",
       "      <td>0.424487</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.896755</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>parent_document_retrieval_chain</td>\n",
       "      <td>10</td>\n",
       "      <td>0.145341</td>\n",
       "      <td>365018</td>\n",
       "      <td>150980</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014534</td>\n",
       "      <td>36501.8</td>\n",
       "      <td>15098.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.806190</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>0.445128</td>\n",
       "      <td>0.317317</td>\n",
       "      <td>0.883523</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ensemble_retrieval_chain</td>\n",
       "      <td>10</td>\n",
       "      <td>0.283593</td>\n",
       "      <td>1023190</td>\n",
       "      <td>216857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028359</td>\n",
       "      <td>102319.0</td>\n",
       "      <td>21685.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.851429</td>\n",
       "      <td>0.889890</td>\n",
       "      <td>0.767724</td>\n",
       "      <td>0.393110</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891511</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                retriever  Total_Runs  Total_Cost  \\\n",
       "0                   naive_retrieval_chain          10    0.243176   \n",
       "0                    bm25_retrieval_chain          10    0.150856   \n",
       "0  contextual_compression_retrieval_chain          10    0.111962   \n",
       "0             multi_query_retrieval_chain          10    0.254773   \n",
       "0         parent_document_retrieval_chain          10    0.145341   \n",
       "0                ensemble_retrieval_chain          10    0.283593   \n",
       "\n",
       "   Total_Input_Tokens  Total_Output_Tokens  Total_Latency_Sec  \\\n",
       "0              659319               240463                  0   \n",
       "0              392780               153232                  0   \n",
       "0              280371               116510                  0   \n",
       "0              805097               223347                  0   \n",
       "0              365018               150980                  0   \n",
       "0             1023190               216857                  0   \n",
       "\n",
       "   Avg_Cost_Per_Run  Avg_Input_Tokens_Per_Run  Avg_Output_Tokens_Per_Run  \\\n",
       "0          0.024318                   65931.9                    24046.3   \n",
       "0          0.015086                   39278.0                    15323.2   \n",
       "0          0.011196                   28037.1                    11651.0   \n",
       "0          0.025477                   80509.7                    22334.7   \n",
       "0          0.014534                   36501.8                    15098.0   \n",
       "0          0.028359                  102319.0                    21685.7   \n",
       "\n",
       "   Avg_Latency_Sec  context_recall  llm_context_precision_without_reference  \\\n",
       "0                0        0.794524                                 0.918197   \n",
       "0                0        0.822857                                 0.913889   \n",
       "0                0        0.639524                                 0.983333   \n",
       "0                0        0.844524                                 0.908392   \n",
       "0                0        0.806190                                 0.933333   \n",
       "0                0        0.851429                                 0.889890   \n",
       "\n",
       "   llm_context_precision_with_reference  \\\n",
       "0                              0.777317   \n",
       "0                              0.683333   \n",
       "0                              0.733333   \n",
       "0                              0.850487   \n",
       "0                              0.858333   \n",
       "0                              0.767724   \n",
       "\n",
       "   non_llm_context_precision_with_reference  context_entity_recall  \\\n",
       "0                                  0.392897               0.320769   \n",
       "0                                  0.413889               0.434487   \n",
       "0                                  0.508333               0.449487   \n",
       "0                                  0.359511               0.424487   \n",
       "0                                  0.255556               0.445128   \n",
       "0                                  0.393110               0.474359   \n",
       "\n",
       "   noise_sensitivity_relevant  faithfulness  faithful_rate  \n",
       "0                    0.223647      0.808040            1.0  \n",
       "0                    0.379310      0.870687            1.0  \n",
       "0                    0.406235      0.782676            1.0  \n",
       "0                    0.454545      0.896755            1.0  \n",
       "0                    0.317317      0.883523            1.0  \n",
       "0                    0.000000      0.891511            1.0  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import ragas_rank_retrievers\n",
    "importlib.reload(ragas_rank_retrievers)\n",
    "from ragas_rank_retrievers import RetrieverRanker\n",
    "\n",
    "ranker = RetrieverRanker('ragas_retriever_raw_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final outcome of the Ragas Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranker.print_available_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Retriever</th>\n",
       "      <th>Key Metric</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overall Winner</td>\n",
       "      <td>Parent Document</td>\n",
       "      <td>Score: 0.729</td>\n",
       "      <td>Best balanced performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Budget Option</td>\n",
       "      <td>Contextual Compression</td>\n",
       "      <td>Cost: $0.0112</td>\n",
       "      <td>Lowest cost per run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quality Leader</td>\n",
       "      <td>Multi Query</td>\n",
       "      <td>Quality: 0.864</td>\n",
       "      <td>Highest average across 3 quality metrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Production Ready</td>\n",
       "      <td>Parent Document</td>\n",
       "      <td>Score: 0.540</td>\n",
       "      <td>Meets minimum thresholds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Category               Retriever      Key Metric  \\\n",
       "0    Overall Winner         Parent Document    Score: 0.729   \n",
       "1     Budget Option  Contextual Compression   Cost: $0.0112   \n",
       "2    Quality Leader             Multi Query  Quality: 0.864   \n",
       "3  Production Ready         Parent Document    Score: 0.540   \n",
       "\n",
       "                                Description  \n",
       "0                 Best balanced performance  \n",
       "1                       Lowest cost per run  \n",
       "2  Highest average across 3 quality metrics  \n",
       "3                  Meets minimum thresholds  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.get_recommendations_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>retriever_chain</th>\n",
       "      <th>score</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>llm_context_precision_with_reference</th>\n",
       "      <th>llm_context_precision_without_reference</th>\n",
       "      <th>faithful_rate</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>Avg_Cost_Per_Run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Parent Document</td>\n",
       "      <td>0.7290</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>0.8835</td>\n",
       "      <td>0.8583</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4451</td>\n",
       "      <td>0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Multi Query</td>\n",
       "      <td>0.7278</td>\n",
       "      <td>0.8445</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.8505</td>\n",
       "      <td>0.9084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.0255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0.6126</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8915</td>\n",
       "      <td>0.7677</td>\n",
       "      <td>0.8899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4744</td>\n",
       "      <td>0.0284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bm25</td>\n",
       "      <td>0.6052</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.8707</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4345</td>\n",
       "      <td>0.0151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Contextual Compression</td>\n",
       "      <td>0.5338</td>\n",
       "      <td>0.6395</td>\n",
       "      <td>0.7827</td>\n",
       "      <td>0.7333</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4495</td>\n",
       "      <td>0.0112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.7773</td>\n",
       "      <td>0.9182</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3208</td>\n",
       "      <td>0.0243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank         retriever_chain   score  context_recall  faithfulness  \\\n",
       "0     1         Parent Document  0.7290          0.8062        0.8835   \n",
       "1     2             Multi Query  0.7278          0.8445        0.8968   \n",
       "2     3                Ensemble  0.6126          0.8514        0.8915   \n",
       "3     4                    Bm25  0.6052          0.8229        0.8707   \n",
       "4     5  Contextual Compression  0.5338          0.6395        0.7827   \n",
       "5     6                   Naive  0.4459          0.7945        0.8080   \n",
       "\n",
       "   llm_context_precision_with_reference  \\\n",
       "0                                0.8583   \n",
       "1                                0.8505   \n",
       "2                                0.7677   \n",
       "3                                0.6833   \n",
       "4                                0.7333   \n",
       "5                                0.7773   \n",
       "\n",
       "   llm_context_precision_without_reference  faithful_rate  \\\n",
       "0                                   0.9333            1.0   \n",
       "1                                   0.9084            1.0   \n",
       "2                                   0.8899            1.0   \n",
       "3                                   0.9139            1.0   \n",
       "4                                   0.9833            1.0   \n",
       "5                                   0.9182            1.0   \n",
       "\n",
       "   context_entity_recall  Avg_Cost_Per_Run  \n",
       "0                 0.4451            0.0145  \n",
       "1                 0.4245            0.0255  \n",
       "2                 0.4744            0.0284  \n",
       "3                 0.4345            0.0151  \n",
       "4                 0.4495            0.0112  \n",
       "5                 0.3208            0.0243  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.get_rankings_table('weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retriever_chain</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>llm_context_precision_with_reference</th>\n",
       "      <th>llm_context_precision_without_reference</th>\n",
       "      <th>non_llm_context_precision_with_reference</th>\n",
       "      <th>faithful_rate</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>noise_sensitivity_relevant</th>\n",
       "      <th>Avg_Cost_Per_Run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.7773</td>\n",
       "      <td>0.9182</td>\n",
       "      <td>0.3929</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3208</td>\n",
       "      <td>0.2236</td>\n",
       "      <td>0.0243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bm25</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.8707</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.4139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4345</td>\n",
       "      <td>0.3793</td>\n",
       "      <td>0.0151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contextual Compression</td>\n",
       "      <td>0.6395</td>\n",
       "      <td>0.7827</td>\n",
       "      <td>0.7333</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.5083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4495</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>0.0112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multi Query</td>\n",
       "      <td>0.8445</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.8505</td>\n",
       "      <td>0.9084</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.0255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parent Document</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>0.8835</td>\n",
       "      <td>0.8583</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4451</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8915</td>\n",
       "      <td>0.7677</td>\n",
       "      <td>0.8899</td>\n",
       "      <td>0.3931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4744</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          retriever_chain  context_recall  faithfulness  \\\n",
       "0                   Naive          0.7945        0.8080   \n",
       "1                    Bm25          0.8229        0.8707   \n",
       "2  Contextual Compression          0.6395        0.7827   \n",
       "3             Multi Query          0.8445        0.8968   \n",
       "4         Parent Document          0.8062        0.8835   \n",
       "5                Ensemble          0.8514        0.8915   \n",
       "\n",
       "   llm_context_precision_with_reference  \\\n",
       "0                                0.7773   \n",
       "1                                0.6833   \n",
       "2                                0.7333   \n",
       "3                                0.8505   \n",
       "4                                0.8583   \n",
       "5                                0.7677   \n",
       "\n",
       "   llm_context_precision_without_reference  \\\n",
       "0                                   0.9182   \n",
       "1                                   0.9139   \n",
       "2                                   0.9833   \n",
       "3                                   0.9084   \n",
       "4                                   0.9333   \n",
       "5                                   0.8899   \n",
       "\n",
       "   non_llm_context_precision_with_reference  faithful_rate  \\\n",
       "0                                    0.3929            1.0   \n",
       "1                                    0.4139            1.0   \n",
       "2                                    0.5083            1.0   \n",
       "3                                    0.3595            1.0   \n",
       "4                                    0.2556            1.0   \n",
       "5                                    0.3931            1.0   \n",
       "\n",
       "   context_entity_recall  noise_sensitivity_relevant  Avg_Cost_Per_Run  \n",
       "0                 0.3208                      0.2236            0.0243  \n",
       "1                 0.4345                      0.3793            0.0151  \n",
       "2                 0.4495                      0.4062            0.0112  \n",
       "3                 0.4245                      0.4545            0.0255  \n",
       "4                 0.4451                      0.3173            0.0145  \n",
       "5                 0.4744                      0.0000            0.0284  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.get_metrics_comparison_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weighted_rank</th>\n",
       "      <th>weighted_score</th>\n",
       "      <th>quality_first_rank</th>\n",
       "      <th>quality_first_score</th>\n",
       "      <th>balanced_rank</th>\n",
       "      <th>balanced_score</th>\n",
       "      <th>production_ready_rank</th>\n",
       "      <th>production_ready_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retriever</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bm25</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6052</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contextual Compression</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7847</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensemble</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6126</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7501</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8605</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Query</th>\n",
       "      <td>2</td>\n",
       "      <td>0.7278</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7918</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0665</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive</th>\n",
       "      <td>6</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>6</td>\n",
       "      <td>0.7481</td>\n",
       "      <td>6</td>\n",
       "      <td>0.7065</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parent Document</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7290</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8509</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        weighted_rank  weighted_score  quality_first_rank  \\\n",
       "retriever                                                                   \n",
       "Bm25                                4          0.6052                   2   \n",
       "Contextual Compression              5          0.5338                   4   \n",
       "Ensemble                            3          0.6126                   5   \n",
       "Multi Query                         2          0.7278                   3   \n",
       "Naive                               6          0.4459                   6   \n",
       "Parent Document                     1          0.7290                   1   \n",
       "\n",
       "                        quality_first_score  balanced_rank  balanced_score  \\\n",
       "retriever                                                                    \n",
       "Bm25                                 0.8000              4          0.8241   \n",
       "Contextual Compression               0.7847              5          0.7125   \n",
       "Ensemble                             0.7501              3          0.8605   \n",
       "Multi Query                          0.7918              2          1.0665   \n",
       "Naive                                0.7481              6          0.7065   \n",
       "Parent Document                      0.8509              1          1.1147   \n",
       "\n",
       "                        production_ready_rank  production_ready_score  \n",
       "retriever                                                              \n",
       "Bm25                                        5                  0.0000  \n",
       "Contextual Compression                      6                  0.0000  \n",
       "Ensemble                                    3                  0.2607  \n",
       "Multi Query                                 2                  0.3921  \n",
       "Naive                                       4                  0.2005  \n",
       "Parent Document                             1                  0.5397  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.get_algorithm_comparison_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
